{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: C:\\Users\\Cyril\\Desktop\\Code\\MIPLab-TeamCEE-DeepLearningforBiomed\\DATA\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd() / \"../code\"))\n",
    "from os import environ\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import logging\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from training import training_loop\n",
    "from utils import balanced_data_shuffle, get_df_raw_data\n",
    "from models import LinearLayer\n",
    "## Data path ##\n",
    "DATA_PATH = (Path.cwd().parent / \"DATA\").resolve()\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "DATA_PATH = str(DATA_PATH)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# set deterministic behavior\n",
    "seed = 53498298\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "torch.use_deterministic_algorithms(True)\n",
    "environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "# environ[\"WANDB_MODE\"] = \"disabled\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # data\n",
    "    \"stratify\": True,\n",
    "    \"validation_split\": 0,\n",
    "    # general\n",
    "    \"epochs\": 30,\n",
    "    \"batch_size\": 32,\n",
    "    \"lr\": 1e-4,\n",
    "    \"use_scheduler\": False,\n",
    "    \"do_early_stopping\": False,\n",
    "    \"patience\": 10,\n",
    "    \"best_loss\": 10,\n",
    "    # model\n",
    "    \"d_model_input\": 400,\n",
    "    \"d_model_intermediate\": [1000],\n",
    "    \"d_model_task_output\": 8,\n",
    "    \"d_model_fingerprint_output\": None,  # needs to be determined from data\n",
    "    \"dropout\": 0,\n",
    "    \"attention_dropout\": 0.1,\n",
    "    \"num_heads\": 1,\n",
    "    # optimizer\n",
    "    \"lambda_si\": 0.5,\n",
    "    \"lambda_td\": 0.5,\n",
    "    \"weight_decay\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDs = [\n",
    "    100307,\n",
    "    117122,\n",
    "    131722,\n",
    "    153025,\n",
    "    211720,\n",
    "    100408,\n",
    "    118528,\n",
    "    133019,\n",
    "    154734,\n",
    "    212318,\n",
    "    101107,\n",
    "    118730,\n",
    "    133928,\n",
    "    156637,\n",
    "    214423,\n",
    "    101309,\n",
    "    118932,\n",
    "    135225,\n",
    "    159340,\n",
    "    221319,\n",
    "    101915,\n",
    "    120111,\n",
    "    135932,\n",
    "    160123,\n",
    "    239944,\n",
    "    103111,\n",
    "    122317,\n",
    "    136833,\n",
    "    161731,\n",
    "    245333,\n",
    "    103414,\n",
    "    122620,\n",
    "    138534,\n",
    "    162733,\n",
    "    280739,\n",
    "    103818,\n",
    "    123117,\n",
    "    139637,\n",
    "    163129,\n",
    "    298051,\n",
    "    105014,\n",
    "    123925,\n",
    "    140925,\n",
    "    176542,\n",
    "    366446,\n",
    "    105115,\n",
    "    124422,\n",
    "    144832,\n",
    "    178950,\n",
    "    397760,\n",
    "    106016,\n",
    "    125525,\n",
    "    146432,\n",
    "    188347,\n",
    "    414229,\n",
    "    108828,\n",
    "    126325,\n",
    "    147737,\n",
    "    189450,\n",
    "    499566,\n",
    "    110411,\n",
    "    127630,\n",
    "    148335,\n",
    "    190031,\n",
    "    654754,\n",
    "    111312,\n",
    "    127933,\n",
    "    148840,\n",
    "    192540,\n",
    "    672756,\n",
    "    111716,\n",
    "    128127,\n",
    "    149337,\n",
    "    196750,\n",
    "    751348,\n",
    "    113619,\n",
    "    128632,\n",
    "    149539,\n",
    "    198451,\n",
    "    756055,\n",
    "    113922,\n",
    "    129028,\n",
    "    149741,\n",
    "    199655,\n",
    "    792564,\n",
    "    114419,\n",
    "    130013,\n",
    "    151223,\n",
    "    201111,\n",
    "    856766,\n",
    "    115320,\n",
    "    130316,\n",
    "    151526,\n",
    "    208226,\n",
    "    857263,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _show_df_distribution(df):\n",
    "    print(\"Number of samples:\", len(df))\n",
    "    print(\"Unique subjects:\", df[\"subject_id\"].nunique())\n",
    "    print(\"Unique tasks:\", df[\"task\"].nunique())\n",
    "    print(\"*\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining train and test dataframes from all subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subjects: 95\n",
      "Number of tasks: 8\n",
      "Subjects present in train set but not in test set:\n",
      "set()\n",
      "Train set:\n",
      "Number of samples: 760\n",
      "Unique subjects: 95\n",
      "Unique tasks: 8\n",
      "**************************************************\n",
      "Test set:\n",
      "Number of samples: 758\n",
      "Unique subjects: 95\n",
      "Unique tasks: 8\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "# data_dict_train, data_dict_test = get_dict_raw_data(DATA_PATH, IDs[0:3])\n",
    "data_df_train, data_df_test = get_df_raw_data(DATA_PATH, IDs[:])\n",
    "\n",
    "train_dataframe, valid_dataframe = balanced_data_shuffle(\n",
    "    data_df_train,\n",
    "    val_frac=config[\"validation_split\"],\n",
    "    stratify=config[\"stratify\"],\n",
    ")\n",
    "NUM_SUBJECTS = len(data_df_train[\"subject_id\"].unique())\n",
    "print(f\"Number of subjects: {NUM_SUBJECTS}\")\n",
    "NUM_TASKS = data_df_train[\"task\"].nunique()\n",
    "print(f\"Number of tasks: {NUM_TASKS}\")\n",
    "\n",
    "#\n",
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "#         label encoding\n",
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "enc_labels = LabelEncoder()\n",
    "enc_tasks = LabelEncoder()\n",
    "\n",
    "enc_labels.fit(data_df_train[\"subject_id\"].tolist())\n",
    "enc_tasks.fit(data_df_train[\"task\"].tolist())\n",
    "\n",
    "enc_train_label_encodings = enc_labels.transform(\n",
    "    train_dataframe[\"subject_id\"].tolist()\n",
    ")\n",
    "enc_train_task_encodings = enc_tasks.transform(\n",
    "    train_dataframe[\"task\"].tolist()\n",
    ")\n",
    "\n",
    "enc_test_label_encodings = enc_labels.transform(\n",
    "    data_df_test[\"subject_id\"].tolist()\n",
    ")\n",
    "enc_test_task_encodings = enc_tasks.transform(\n",
    "    data_df_test[\"task\"].tolist()\n",
    ")\n",
    "\n",
    "train_dataframe[\"enc_label_id\"] = enc_train_label_encodings\n",
    "train_dataframe[\"enc_task\"] = enc_train_task_encodings\n",
    "data_df_test[\"enc_label_id\"] = enc_test_label_encodings\n",
    "data_df_test[\"enc_task\"] = enc_test_task_encodings\n",
    "\n",
    "print(\"Subjects present in train set but not in test set:\")\n",
    "overlap_set = set(train_dataframe[\"subject_id\"].unique()) - set(\n",
    "    data_df_test[\"subject_id\"].unique()\n",
    ")\n",
    "print(overlap_set)\n",
    "if len(overlap_set) != 0:\n",
    "    print(\"WARNING: subjects present in train set but not in test set\")\n",
    "\n",
    "print(\"Train set:\")\n",
    "_show_df_distribution(train_dataframe)\n",
    "print(\"Test set:\")\n",
    "_show_df_distribution(data_df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(\n",
    "        np.array(train_dataframe[\"mat\"].tolist()).astype(np.float32)\n",
    "    ),\n",
    "    torch.tensor(train_dataframe[\"enc_label_id\"].to_numpy()),\n",
    "    torch.tensor(train_dataframe[\"enc_task\"].to_numpy()),\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=config[\"batch_size\"], shuffle=True\n",
    ")\n",
    "\n",
    "test_dataset = TensorDataset(\n",
    "    torch.tensor(\n",
    "        np.array(data_df_test[\"mat\"].tolist()).astype(np.float32)\n",
    "    ),\n",
    "    torch.tensor(data_df_test[\"enc_label_id\"].to_numpy()),\n",
    "    torch.tensor(data_df_test[\"enc_task\"].to_numpy()),\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=config[\"batch_size\"], shuffle=False\n",
    ")\n",
    "\n",
    "valid_loader = None\n",
    "if valid_dataframe is not None:\n",
    "    enc_valid_label_encodings = enc_labels.transform(\n",
    "        valid_dataframe[\"subject_id\"].tolist()\n",
    "    )\n",
    "    enc_valid_task_encodings = enc_tasks.transform(\n",
    "        valid_dataframe[\"task\"].tolist()\n",
    "    )\n",
    "    valid_dataframe[\"enc_label_id\"] = enc_valid_label_encodings\n",
    "    valid_dataframe[\"enc_task\"] = enc_valid_task_encodings\n",
    "    print(\"Subjects present in validation set but not in train set:\")\n",
    "    overlap_set = set(valid_dataframe[\"subject_id\"].unique()) - set(\n",
    "        train_dataframe[\"subject_id\"].unique()\n",
    "    )\n",
    "    print(overlap_set)\n",
    "    if len(overlap_set) != 0:\n",
    "        print(\n",
    "            \"WARNING: subjects present in validation set but not in train set\"\n",
    "        )\n",
    "    valid_dataset = TensorDataset(\n",
    "        torch.tensor(\n",
    "            np.array(valid_dataframe[\"mat\"].tolist()).astype(np.float32)\n",
    "        ),\n",
    "        torch.tensor(valid_dataframe[\"enc_label_id\"].to_numpy()),\n",
    "        torch.tensor(valid_dataframe[\"enc_task\"].to_numpy()),\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, batch_size=config[\"batch_size\"], shuffle=False\n",
    "    )\n",
    "    print(\"Validation set:\")\n",
    "    _show_df_distribution(valid_dataframe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Using cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "INFO:wandb:Watching\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30 - loss_total: 3.2544 - acc: SI 1.48% / TD 43.53%\n",
      " - (2.44s/epoch)\n",
      "Epoch: 2/30 - loss_total: 2.7205 - acc: SI 32.60% / TD 76.17%\n",
      " - (1.94s/epoch)\n",
      "Epoch: 3/30 - loss_total: 2.2973 - acc: SI 45.79% / TD 89.45%\n",
      " - (1.93s/epoch)\n",
      "Epoch: 4/30 - loss_total: 1.4951 - acc: SI 80.86% / TD 93.36%\n",
      " - (1.91s/epoch)\n",
      "Epoch: 5/30 - loss_total: 0.5752 - acc: SI 98.70% / TD 97.35%\n",
      " - (1.92s/epoch)\n",
      "Epoch: 6/30 - loss_total: 0.3768 - acc: SI 99.87% / TD 98.57%\n",
      " - (1.91s/epoch)\n",
      "Epoch: 7/30 - loss_total: 0.2257 - acc: SI 100.00% / TD 99.35%\n",
      " - (1.91s/epoch)\n",
      "Epoch: 8/30 - loss_total: 0.1332 - acc: SI 100.00% / TD 99.87%\n",
      " - (1.91s/epoch)\n",
      "Epoch: 9/30 - loss_total: 0.0801 - acc: SI 100.00% / TD 100.00%\n",
      " - (1.91s/epoch)\n",
      "Epoch: 10/30 - loss_total: 0.0614 - acc: SI 100.00% / TD 100.00%\n",
      " - (1.91s/epoch)\n",
      "Epoch: 11/30 - loss_total: 0.0501 - acc: SI 100.00% / TD 100.00%\n",
      " - (1.91s/epoch)\n",
      "Epoch: 12/30 - loss_total: 0.0432 - acc: SI 100.00% / TD 100.00%\n",
      " - (1.91s/epoch)\n",
      "Epoch: 13/30 - loss_total: 0.0384 - acc: SI 100.00% / TD 100.00%\n",
      " - (1.91s/epoch)\n",
      "Epoch: 14/30 - loss_total: 0.0347 - acc: SI 100.00% / TD 100.00%\n",
      " - (1.91s/epoch)\n",
      "Epoch: 15/30 - loss_total: 0.0317 - acc: SI 100.00% / TD 100.00%\n",
      " - (1.90s/epoch)\n",
      "Epoch: 16/30 - loss_total: 0.0292 - acc: SI 100.00% / TD 100.00%\n",
      " - (1.91s/epoch)\n",
      "Epoch: 17/30 - loss_total: 0.0270 - acc: SI 100.00% / TD 100.00%\n",
      " - (2.00s/epoch)\n",
      "Epoch: 18/30 - loss_total: 0.0251 - acc: SI 100.00% / TD 100.00%\n",
      " - (1.95s/epoch)\n",
      "Epoch: 19/30 - loss_total: 0.0235 - acc: SI 100.00% / TD 100.00%\n",
      " - (1.94s/epoch)\n",
      "Epoch: 20/30 - loss_total: 0.0220 - acc: SI 100.00% / TD 100.00%\n",
      " - (1.95s/epoch)\n",
      "Epoch: 21/30 - loss_total: 0.0206 - acc: SI 100.00% / TD 100.00%\n",
      " - (1.95s/epoch)\n",
      "Epoch: 22/30 - loss_total: 0.0195 - acc: SI 100.00% / TD 100.00%\n",
      " - (1.95s/epoch)\n",
      "Epoch: 23/30 - loss_total: 0.0183 - acc: SI 100.00% / TD 100.00%\n",
      " - (1.95s/epoch)\n",
      "Epoch: 24/30 - loss_total: 0.0171 - acc: SI 100.00% / TD 100.00%\n",
      " - (1.95s/epoch)\n",
      "Epoch: 25/30 - loss_total: 0.0163 - acc: SI 100.00% / TD 100.00%\n",
      " - (1.95s/epoch)\n",
      "Epoch: 26/30 - loss_total: 0.0155 - acc: SI 100.00% / TD 100.00%\n",
      " - (1.97s/epoch)\n",
      "Epoch: 27/30 - loss_total: 0.0147 - acc: SI 100.00% / TD 100.00%\n",
      " - (1.95s/epoch)\n",
      "Epoch: 28/30 - loss_total: 0.0141 - acc: SI 100.00% / TD 100.00%\n",
      " - (1.95s/epoch)\n",
      "Epoch: 29/30 - loss_total: 0.0133 - acc: SI 100.00% / TD 100.00%\n",
      " - (1.95s/epoch)\n",
      "Epoch: 30/30 - loss_total: 0.0127 - acc: SI 100.00% / TD 100.00%\n",
      " - (1.94s/epoch)\n",
      "______________________________\n",
      "Final test loss: 0.2287 - acc: SI 99.48% / TD 94.74% - f1: SI 0.9688 / TD 0.9456\n",
      "______________________________\n",
      "Running DeepLIFT\n",
      "SI attributions shape :  torch.Size([24, 400, 400])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cyril\\anaconda3\\envs\\DLbiomed\\lib\\site-packages\\captum\\_utils\\gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Cyril\\anaconda3\\envs\\DLbiomed\\lib\\site-packages\\captum\\attr\\_core\\deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n",
      "               activations. The hooks and attributes will be removed\n",
      "            after the attribution is finished\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DeepLIFT for task REST1\n",
      "TD attributions shape :  torch.Size([24, 400, 400])\n",
      "Running DeepLIFT for task EMOTION\n",
      "TD attributions shape :  torch.Size([24, 400, 400])\n",
      "Running DeepLIFT for task GAMBLING\n",
      "TD attributions shape :  torch.Size([24, 400, 400])\n",
      "Running DeepLIFT for task LANGUAGE\n",
      "TD attributions shape :  torch.Size([24, 400, 400])\n",
      "Running DeepLIFT for task MOTOR\n",
      "TD attributions shape :  torch.Size([24, 400, 400])\n",
      "Running DeepLIFT for task RELATIONAL\n",
      "TD attributions shape :  torch.Size([24, 400, 400])\n",
      "Running DeepLIFT for task SOCIAL\n",
      "TD attributions shape :  torch.Size([24, 400, 400])\n",
      "Running DeepLIFT for task WM\n",
      "TD attributions shape :  torch.Size([24, 400, 400])\n",
      "Finished Training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'epoch': 30,\n",
       " 'loss_total': [3.254369010527929,\n",
       "  2.720523029565811,\n",
       "  2.2972715298334756,\n",
       "  1.4951352725426357,\n",
       "  0.5752105688055357,\n",
       "  0.3767905669907729,\n",
       "  0.225746746485432,\n",
       "  0.13317152919868627,\n",
       "  0.08012466412037611,\n",
       "  0.061437145651628576,\n",
       "  0.05007062417765459,\n",
       "  0.04319208969051639,\n",
       "  0.03835362164924542,\n",
       "  0.03474711145584782,\n",
       "  0.03170205761368076,\n",
       "  0.029246449160079162,\n",
       "  0.027028008131310344,\n",
       "  0.025143034057691693,\n",
       "  0.02350838699688514,\n",
       "  0.022005045398448903,\n",
       "  0.02062260825186968,\n",
       "  0.019507000300412376,\n",
       "  0.018324597040191293,\n",
       "  0.01712477017038812,\n",
       "  0.016329101674879592,\n",
       "  0.015474456634062031,\n",
       "  0.014706248766742647,\n",
       "  0.014063984815341731,\n",
       "  0.013315659947693348,\n",
       "  0.012716010639754435],\n",
       " 'loss_si': [4.669316093126933,\n",
       "  4.215941846370697,\n",
       "  3.8209771116574607,\n",
       "  2.5727484623591104,\n",
       "  0.9793021803100904,\n",
       "  0.673474540313085,\n",
       "  0.39477939158678055,\n",
       "  0.24367517046630383,\n",
       "  0.15120556764304638,\n",
       "  0.1165308402851224,\n",
       "  0.09486217331141233,\n",
       "  0.0816636960953474,\n",
       "  0.07231672666966915,\n",
       "  0.06546235193187992,\n",
       "  0.059640872137000166,\n",
       "  0.05492180089155833,\n",
       "  0.05065684641400973,\n",
       "  0.04704879084601998,\n",
       "  0.04391792928799987,\n",
       "  0.04103838341931502,\n",
       "  0.03837640661125382,\n",
       "  0.03624329390004277,\n",
       "  0.03396207947904865,\n",
       "  0.03165472694672644,\n",
       "  0.030122094477216404,\n",
       "  0.028486709653710324,\n",
       "  0.027005693564812343,\n",
       "  0.02577248013888796,\n",
       "  0.024329844474171598,\n",
       "  0.02318664368552466],\n",
       " 'loss_td': [1.839421957731247,\n",
       "  1.2251041879256566,\n",
       "  0.7735659802953402,\n",
       "  0.41752208210527897,\n",
       "  0.17111895264436802,\n",
       "  0.08010659242669742,\n",
       "  0.0567141038676103,\n",
       "  0.022667891113087535,\n",
       "  0.009043760617108395,\n",
       "  0.006343450707693894,\n",
       "  0.005279075587168336,\n",
       "  0.004720483722242837,\n",
       "  0.0043905168131459504,\n",
       "  0.00403187107682849,\n",
       "  0.0037632426149987928,\n",
       "  0.0035710971957693496,\n",
       "  0.003399170052337771,\n",
       "  0.0032372775798042617,\n",
       "  0.003098844395329555,\n",
       "  0.002971707241764913,\n",
       "  0.0028688098827842623,\n",
       "  0.00277070704032667,\n",
       "  0.0026871143491007388,\n",
       "  0.0025948132679332048,\n",
       "  0.002536109347905343,\n",
       "  0.002462203895750766,\n",
       "  0.0024068038328550756,\n",
       "  0.0023554894141852856,\n",
       "  0.002301475430916374,\n",
       "  0.0022453776618931442],\n",
       " 'acc_si': [1.4756944444444446,\n",
       "  32.59548611111111,\n",
       "  45.78993055555556,\n",
       "  80.859375,\n",
       "  98.69791666666666,\n",
       "  99.86979166666666,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0],\n",
       " 'acc_td': [],\n",
       " 'val-loss_total': [],\n",
       " 'val-loss_si': [],\n",
       " 'val-loss_td': [],\n",
       " 'val-acc_si': [],\n",
       " 'val-acc_td': [],\n",
       " 'LR': [],\n",
       " 'test_acc_si': 99.47916666666666,\n",
       " 'test_acc_td': 94.74431818181817,\n",
       " 'test_f1_si': 0.9688176638176639,\n",
       " 'test_f1_td': 0.9455909541847043}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "#         Model\n",
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# list all available torch devices\n",
    "device_list = [\"cpu\"] + [\n",
    "    f\"cuda:{i}\" for i in range(torch.cuda.device_count())\n",
    "]\n",
    "device = device_list[-1] if torch.cuda.is_available() else device_list[0]\n",
    "print(f\"Using device: {device}\")\n",
    "model = LinearLayer(\n",
    "    output_size_tasks=NUM_TASKS,\n",
    "    output_size_subjects=NUM_SUBJECTS,\n",
    "    input_size=config[\"d_model_input\"],\n",
    "    intermediate_size=config[\"d_model_intermediate\"],\n",
    "    dropout=config[\"dropout\"],\n",
    ").to(device)\n",
    "wandb_run_name = (\n",
    "    \"LinearSplit_best_repro_FINAL\"\n",
    ")\n",
    "\n",
    "\n",
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "#         training\n",
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config[\"lr\"],\n",
    "    weight_decay=config[\"weight_decay\"],\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=20, gamma=0.1\n",
    ")\n",
    "\n",
    "training_loop(\n",
    "    config[\"epochs\"],\n",
    "    model,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    device,\n",
    "    config,\n",
    "    scheduler=scheduler if config[\"use_scheduler\"] else None,\n",
    "    save_model=True,\n",
    "    save_attention_weights=False,\n",
    "    test_loader=test_loader,\n",
    "    run_name=wandb_run_name,\n",
    "    job_name=\"LinearSplit\",\n",
    "    use_deeplift=True,\n",
    "    use_early_stopping=config[\"do_early_stopping\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLbiomed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
