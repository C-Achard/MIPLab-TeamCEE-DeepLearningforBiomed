{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: C:\\Users\\Cyril\\Desktop\\Code\\MIPLab-TeamCEE-DeepLearningforBiomed\\DATA\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd() / \"../code\"))\n",
    "from os import environ\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import logging\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from training import training_loop\n",
    "from utils import balanced_data_shuffle, get_df_raw_data\n",
    "from models import LinearLayerShared\n",
    "## Data path ##\n",
    "DATA_PATH = (Path.cwd().parent / \"DATA\").resolve()\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "DATA_PATH = str(DATA_PATH)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# set deterministic behavior\n",
    "seed = 53498298\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "torch.use_deterministic_algorithms(True)\n",
    "environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # data\n",
    "    \"stratify\": True,\n",
    "    \"validation_split\": 0,\n",
    "    # general\n",
    "    \"epochs\": 30,\n",
    "    \"batch_size\": 32,\n",
    "    \"lr\": 1e-4,\n",
    "    \"use_scheduler\": False,\n",
    "    \"do_early_stopping\": False,\n",
    "    \"patience\": 10,\n",
    "    \"best_loss\": 10,\n",
    "    # model\n",
    "    \"d_model_input\": 400,\n",
    "    \"d_model_intermediate\": [1000],\n",
    "    \"d_model_task_output\": 8,\n",
    "    \"d_model_fingerprint_output\": None,  # needs to be determined from data\n",
    "    \"dropout\": 0,\n",
    "    \"attention_dropout\": 0.1,\n",
    "    \"num_heads\": 1,\n",
    "    # optimizer\n",
    "    \"lambda_si\": 0.5,\n",
    "    \"lambda_td\": 0.5,\n",
    "    \"weight_decay\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDs = [\n",
    "    100307,\n",
    "    117122,\n",
    "    131722,\n",
    "    153025,\n",
    "    211720,\n",
    "    100408,\n",
    "    118528,\n",
    "    133019,\n",
    "    154734,\n",
    "    212318,\n",
    "    101107,\n",
    "    118730,\n",
    "    133928,\n",
    "    156637,\n",
    "    214423,\n",
    "    101309,\n",
    "    118932,\n",
    "    135225,\n",
    "    159340,\n",
    "    221319,\n",
    "    101915,\n",
    "    120111,\n",
    "    135932,\n",
    "    160123,\n",
    "    239944,\n",
    "    103111,\n",
    "    122317,\n",
    "    136833,\n",
    "    161731,\n",
    "    245333,\n",
    "    103414,\n",
    "    122620,\n",
    "    138534,\n",
    "    162733,\n",
    "    280739,\n",
    "    103818,\n",
    "    123117,\n",
    "    139637,\n",
    "    163129,\n",
    "    298051,\n",
    "    105014,\n",
    "    123925,\n",
    "    140925,\n",
    "    176542,\n",
    "    366446,\n",
    "    105115,\n",
    "    124422,\n",
    "    144832,\n",
    "    178950,\n",
    "    397760,\n",
    "    106016,\n",
    "    125525,\n",
    "    146432,\n",
    "    188347,\n",
    "    414229,\n",
    "    108828,\n",
    "    126325,\n",
    "    147737,\n",
    "    189450,\n",
    "    499566,\n",
    "    110411,\n",
    "    127630,\n",
    "    148335,\n",
    "    190031,\n",
    "    654754,\n",
    "    111312,\n",
    "    127933,\n",
    "    148840,\n",
    "    192540,\n",
    "    672756,\n",
    "    111716,\n",
    "    128127,\n",
    "    149337,\n",
    "    196750,\n",
    "    751348,\n",
    "    113619,\n",
    "    128632,\n",
    "    149539,\n",
    "    198451,\n",
    "    756055,\n",
    "    113922,\n",
    "    129028,\n",
    "    149741,\n",
    "    199655,\n",
    "    792564,\n",
    "    114419,\n",
    "    130013,\n",
    "    151223,\n",
    "    201111,\n",
    "    856766,\n",
    "    115320,\n",
    "    130316,\n",
    "    151526,\n",
    "    208226,\n",
    "    857263,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _show_df_distribution(df):\n",
    "    print(\"Number of samples:\", len(df))\n",
    "    print(\"Unique subjects:\", df[\"subject_id\"].nunique())\n",
    "    print(\"Unique tasks:\", df[\"task\"].nunique())\n",
    "    print(\"*\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining train and test dataframes from all subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subjects: 95\n",
      "Number of tasks: 8\n",
      "Subjects present in train set but not in test set:\n",
      "set()\n",
      "Train set:\n",
      "Number of samples: 760\n",
      "Unique subjects: 95\n",
      "Unique tasks: 8\n",
      "**************************************************\n",
      "Test set:\n",
      "Number of samples: 758\n",
      "Unique subjects: 95\n",
      "Unique tasks: 8\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "# data_dict_train, data_dict_test = get_dict_raw_data(DATA_PATH, IDs[0:3])\n",
    "data_df_train, data_df_test = get_df_raw_data(DATA_PATH, IDs[:])\n",
    "\n",
    "train_dataframe, valid_dataframe = balanced_data_shuffle(\n",
    "    data_df_train,\n",
    "    val_frac=config[\"validation_split\"],\n",
    "    stratify=config[\"stratify\"],\n",
    ")\n",
    "NUM_SUBJECTS = len(data_df_train[\"subject_id\"].unique())\n",
    "print(f\"Number of subjects: {NUM_SUBJECTS}\")\n",
    "NUM_TASKS = data_df_train[\"task\"].nunique()\n",
    "print(f\"Number of tasks: {NUM_TASKS}\")\n",
    "\n",
    "#\n",
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "#         label encoding\n",
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "enc_labels = LabelEncoder()\n",
    "enc_tasks = LabelEncoder()\n",
    "\n",
    "enc_labels.fit(data_df_train[\"subject_id\"].tolist())\n",
    "enc_tasks.fit(data_df_train[\"task\"].tolist())\n",
    "\n",
    "enc_train_label_encodings = enc_labels.transform(\n",
    "    train_dataframe[\"subject_id\"].tolist()\n",
    ")\n",
    "enc_train_task_encodings = enc_tasks.transform(\n",
    "    train_dataframe[\"task\"].tolist()\n",
    ")\n",
    "\n",
    "enc_test_label_encodings = enc_labels.transform(\n",
    "    data_df_test[\"subject_id\"].tolist()\n",
    ")\n",
    "enc_test_task_encodings = enc_tasks.transform(\n",
    "    data_df_test[\"task\"].tolist()\n",
    ")\n",
    "\n",
    "train_dataframe[\"enc_label_id\"] = enc_train_label_encodings\n",
    "train_dataframe[\"enc_task\"] = enc_train_task_encodings\n",
    "data_df_test[\"enc_label_id\"] = enc_test_label_encodings\n",
    "data_df_test[\"enc_task\"] = enc_test_task_encodings\n",
    "\n",
    "print(\"Subjects present in train set but not in test set:\")\n",
    "overlap_set = set(train_dataframe[\"subject_id\"].unique()) - set(\n",
    "    data_df_test[\"subject_id\"].unique()\n",
    ")\n",
    "print(overlap_set)\n",
    "if len(overlap_set) != 0:\n",
    "    print(\"WARNING: subjects present in train set but not in test set\")\n",
    "\n",
    "print(\"Train set:\")\n",
    "_show_df_distribution(train_dataframe)\n",
    "print(\"Test set:\")\n",
    "_show_df_distribution(data_df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(\n",
    "        np.array(train_dataframe[\"mat\"].tolist()).astype(np.float32)\n",
    "    ),\n",
    "    torch.tensor(train_dataframe[\"enc_label_id\"].to_numpy()),\n",
    "    torch.tensor(train_dataframe[\"enc_task\"].to_numpy()),\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=config[\"batch_size\"], shuffle=True\n",
    ")\n",
    "\n",
    "test_dataset = TensorDataset(\n",
    "    torch.tensor(\n",
    "        np.array(data_df_test[\"mat\"].tolist()).astype(np.float32)\n",
    "    ),\n",
    "    torch.tensor(data_df_test[\"enc_label_id\"].to_numpy()),\n",
    "    torch.tensor(data_df_test[\"enc_task\"].to_numpy()),\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=config[\"batch_size\"], shuffle=False\n",
    ")\n",
    "\n",
    "valid_loader = None\n",
    "if valid_dataframe is not None:\n",
    "    enc_valid_label_encodings = enc_labels.transform(\n",
    "        valid_dataframe[\"subject_id\"].tolist()\n",
    "    )\n",
    "    enc_valid_task_encodings = enc_tasks.transform(\n",
    "        valid_dataframe[\"task\"].tolist()\n",
    "    )\n",
    "    valid_dataframe[\"enc_label_id\"] = enc_valid_label_encodings\n",
    "    valid_dataframe[\"enc_task\"] = enc_valid_task_encodings\n",
    "    print(\"Subjects present in validation set but not in train set:\")\n",
    "    overlap_set = set(valid_dataframe[\"subject_id\"].unique()) - set(\n",
    "        train_dataframe[\"subject_id\"].unique()\n",
    "    )\n",
    "    print(overlap_set)\n",
    "    if len(overlap_set) != 0:\n",
    "        print(\n",
    "            \"WARNING: subjects present in validation set but not in train set\"\n",
    "        )\n",
    "    valid_dataset = TensorDataset(\n",
    "        torch.tensor(\n",
    "            np.array(valid_dataframe[\"mat\"].tolist()).astype(np.float32)\n",
    "        ),\n",
    "        torch.tensor(valid_dataframe[\"enc_label_id\"].to_numpy()),\n",
    "        torch.tensor(valid_dataframe[\"enc_task\"].to_numpy()),\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, batch_size=config[\"batch_size\"], shuffle=False\n",
    "    )\n",
    "    print(\"Validation set:\")\n",
    "    _show_df_distribution(valid_dataframe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Using cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mc-achard\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Cyril\\Desktop\\Code\\MIPLab-TeamCEE-DeepLearningforBiomed\\notebooks\\wandb\\run-20231210_124749-534xz7ws</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/c-achard/DLB-Project/runs/534xz7ws' target=\"_blank\">LinearShared_best_repro_1000</a></strong> to <a href='https://wandb.ai/c-achard/DLB-Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/c-achard/DLB-Project' target=\"_blank\">https://wandb.ai/c-achard/DLB-Project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/c-achard/DLB-Project/runs/534xz7ws' target=\"_blank\">https://wandb.ai/c-achard/DLB-Project/runs/534xz7ws</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30 - loss_total: 3.3751 - acc: SI 1.87% / TD 33.29%\n",
      " - (8.05s/epoch)\n",
      "Epoch: 2/30 - loss_total: 3.0493 - acc: SI 12.80% / TD 76.82%\n",
      " - (2.43s/epoch)\n",
      "Epoch: 3/30 - loss_total: 2.8336 - acc: SI 27.60% / TD 82.12%\n",
      " - (2.40s/epoch)\n",
      "Epoch: 4/30 - loss_total: 2.4820 - acc: SI 45.57% / TD 84.64%\n",
      " - (2.37s/epoch)\n",
      "Epoch: 5/30 - loss_total: 1.8355 - acc: SI 73.39% / TD 93.36%\n",
      " - (2.39s/epoch)\n",
      "Epoch: 6/30 - loss_total: 1.0819 - acc: SI 93.97% / TD 97.40%\n",
      " - (2.38s/epoch)\n",
      "Epoch: 7/30 - loss_total: 0.5439 - acc: SI 99.18% / TD 99.35%\n",
      " - (2.41s/epoch)\n",
      "Epoch: 8/30 - loss_total: 0.2682 - acc: SI 100.00% / TD 99.87%\n",
      " - (2.32s/epoch)\n",
      "Epoch: 9/30 - loss_total: 0.1474 - acc: SI 100.00% / TD 100.00%\n",
      " - (2.32s/epoch)\n",
      "Epoch: 10/30 - loss_total: 0.0902 - acc: SI 100.00% / TD 100.00%\n",
      " - (2.35s/epoch)\n",
      "Epoch: 11/30 - loss_total: 0.0640 - acc: SI 100.00% / TD 100.00%\n",
      " - (2.33s/epoch)\n",
      "Epoch: 12/30 - loss_total: 0.0509 - acc: SI 100.00% / TD 100.00%\n",
      " - (2.39s/epoch)\n",
      "Epoch: 13/30 - loss_total: 0.0430 - acc: SI 100.00% / TD 100.00%\n",
      " - (2.37s/epoch)\n",
      "Epoch: 14/30 - loss_total: 0.0384 - acc: SI 100.00% / TD 100.00%\n",
      " - (2.36s/epoch)\n",
      "Epoch: 15/30 - loss_total: 0.0344 - acc: SI 100.00% / TD 100.00%\n",
      " - (2.43s/epoch)\n",
      "Epoch: 16/30 - loss_total: 0.0314 - acc: SI 100.00% / TD 100.00%\n",
      " - (2.47s/epoch)\n",
      "Epoch: 17/30 - loss_total: 0.0289 - acc: SI 100.00% / TD 100.00%\n",
      " - (2.45s/epoch)\n",
      "Epoch: 18/30 - loss_total: 0.0267 - acc: SI 100.00% / TD 100.00%\n",
      " - (2.28s/epoch)\n",
      "Epoch: 19/30 - loss_total: 0.0248 - acc: SI 100.00% / TD 100.00%\n",
      " - (2.23s/epoch)\n",
      "Epoch: 20/30 - loss_total: 0.0232 - acc: SI 100.00% / TD 100.00%\n",
      " - (2.41s/epoch)\n",
      "Epoch: 21/30 - loss_total: 0.0217 - acc: SI 100.00% / TD 100.00%\n",
      " - (2.35s/epoch)\n",
      "Epoch: 22/30 - loss_total: 0.0205 - acc: SI 100.00% / TD 100.00%\n",
      " - (2.39s/epoch)\n",
      "Epoch: 23/30 - loss_total: 0.0193 - acc: SI 100.00% / TD 100.00%\n",
      " - (2.27s/epoch)\n",
      "Epoch: 24/30 - loss_total: 0.0182 - acc: SI 100.00% / TD 100.00%\n",
      " - (2.24s/epoch)\n",
      "Epoch: 25/30 - loss_total: 0.0174 - acc: SI 100.00% / TD 100.00%\n",
      " - (2.33s/epoch)\n",
      "Epoch: 26/30 - loss_total: 0.0164 - acc: SI 100.00% / TD 100.00%\n",
      " - (2.32s/epoch)\n",
      "Epoch: 27/30 - loss_total: 0.0157 - acc: SI 100.00% / TD 100.00%\n",
      " - (2.24s/epoch)\n",
      "Epoch: 28/30 - loss_total: 0.0149 - acc: SI 100.00% / TD 100.00%\n",
      " - (2.34s/epoch)\n",
      "Epoch: 29/30 - loss_total: 0.0143 - acc: SI 100.00% / TD 100.00%\n",
      " - (2.32s/epoch)\n",
      "Epoch: 30/30 - loss_total: 0.0137 - acc: SI 100.00% / TD 100.00%\n",
      " - (2.36s/epoch)\n",
      "______________________________\n",
      "Final test loss: 0.2609 - acc: SI 99.87% / TD 96.11% - f1: SI 0.9926 / TD 0.9598\n",
      "______________________________\n",
      "Running DeepLIFT\n",
      "SI attributions shape :  torch.Size([24, 400, 400])\n",
      "Running DeepLIFT for task REST1\n",
      "TD attributions shape :  torch.Size([24, 400, 400])\n",
      "Running DeepLIFT for task EMOTION\n",
      "TD attributions shape :  torch.Size([24, 400, 400])\n",
      "Running DeepLIFT for task GAMBLING\n",
      "TD attributions shape :  torch.Size([24, 400, 400])\n",
      "Running DeepLIFT for task LANGUAGE\n",
      "TD attributions shape :  torch.Size([24, 400, 400])\n",
      "Running DeepLIFT for task MOTOR\n",
      "TD attributions shape :  torch.Size([24, 400, 400])\n",
      "Running DeepLIFT for task RELATIONAL\n",
      "TD attributions shape :  torch.Size([24, 400, 400])\n",
      "Running DeepLIFT for task SOCIAL\n",
      "TD attributions shape :  torch.Size([24, 400, 400])\n",
      "Running DeepLIFT for task WM\n",
      "TD attributions shape :  torch.Size([24, 400, 400])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cyril\\anaconda3\\envs\\DLbiomed\\lib\\site-packages\\captum\\_utils\\gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Cyril\\anaconda3\\envs\\DLbiomed\\lib\\site-packages\\captum\\attr\\_core\\deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n",
      "               activations. The hooks and attributes will be removed\n",
      "            after the attribution is finished\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db820b9f12884c8e864c0ad1fd7996f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch/Epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>Test/acc_si</td><td>▁</td></tr><tr><td>Test/acc_td</td><td>▁</td></tr><tr><td>Test/f1_si</td><td>▁</td></tr><tr><td>Test/f1_td</td><td>▁</td></tr><tr><td>Test/loss_si</td><td>▁</td></tr><tr><td>Test/loss_td</td><td>▁</td></tr><tr><td>Test/total_loss</td><td>▁</td></tr><tr><td>Train/Epoch-acc_si</td><td>▁▂▃▄▆█████████████████████████</td></tr><tr><td>Train/Epoch-acc_td</td><td>▁▆▆▆▇█████████████████████████</td></tr><tr><td>Train/Epoch-loss_si</td><td>██▇▇▆▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train/Epoch-loss_td</td><td>█▇▆▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train/Epoch-total_loss</td><td>█▇▇▆▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train/loss_si</td><td>███▇▇▇▆▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train/loss_td</td><td>█▇▇▆▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train/total_loss</td><td>██▇▇▆▆▅▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch/Epoch</td><td>30</td></tr><tr><td>Test/acc_si</td><td>99.86979</td></tr><tr><td>Test/acc_td</td><td>96.10559</td></tr><tr><td>Test/f1_si</td><td>0.99259</td></tr><tr><td>Test/f1_td</td><td>0.95984</td></tr><tr><td>Test/loss_si</td><td>0.37053</td></tr><tr><td>Test/loss_td</td><td>0.15122</td></tr><tr><td>Test/total_loss</td><td>0.26088</td></tr><tr><td>Train/Epoch-acc_si</td><td>100.0</td></tr><tr><td>Train/Epoch-acc_td</td><td>100.0</td></tr><tr><td>Train/Epoch-loss_si</td><td>0.01845</td></tr><tr><td>Train/Epoch-loss_td</td><td>0.00888</td></tr><tr><td>Train/Epoch-total_loss</td><td>0.01367</td></tr><tr><td>Train/loss_si</td><td>0.01933</td></tr><tr><td>Train/loss_td</td><td>0.00961</td></tr><tr><td>Train/total_loss</td><td>0.01447</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LinearShared_best_repro_1000</strong> at: <a href='https://wandb.ai/c-achard/DLB-Project/runs/534xz7ws' target=\"_blank\">https://wandb.ai/c-achard/DLB-Project/runs/534xz7ws</a><br/>Synced 6 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231210_124749-534xz7ws\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'epoch': 30,\n",
       " 'loss_total': [3.37506032983462,\n",
       "  3.049334943294525,\n",
       "  2.8336475988229117,\n",
       "  2.4820052285989127,\n",
       "  1.8355181366205215,\n",
       "  1.0818914845585823,\n",
       "  0.5439204735060533,\n",
       "  0.26824660412967205,\n",
       "  0.14743680010239282,\n",
       "  0.09020197112113237,\n",
       "  0.06398172847305734,\n",
       "  0.05091770893583695,\n",
       "  0.04299478077640136,\n",
       "  0.038361032803853355,\n",
       "  0.034443633475651346,\n",
       "  0.031444006211434804,\n",
       "  0.028880763954172533,\n",
       "  0.026703542020792764,\n",
       "  0.024795867114638288,\n",
       "  0.023199283595507342,\n",
       "  0.02167311031371355,\n",
       "  0.02047078249355157,\n",
       "  0.019255402963608503,\n",
       "  0.018233076203614473,\n",
       "  0.01735010075693329,\n",
       "  0.016448276466690004,\n",
       "  0.01565183113173892,\n",
       "  0.014926119125448167,\n",
       "  0.014252072937476138,\n",
       "  0.013665597303770483],\n",
       " 'loss_si': [4.6769729653994245,\n",
       "  4.4445816079775495,\n",
       "  4.310614089171092,\n",
       "  4.019453247388204,\n",
       "  3.228326062361399,\n",
       "  1.9565102408329647,\n",
       "  0.9222585732738177,\n",
       "  0.4004078308741252,\n",
       "  0.19587869103997946,\n",
       "  0.11293818522244692,\n",
       "  0.08047678228467703,\n",
       "  0.06416000192984939,\n",
       "  0.054538591454426445,\n",
       "  0.04923788985858361,\n",
       "  0.0442665641506513,\n",
       "  0.040736566142489515,\n",
       "  0.037485928585131965,\n",
       "  0.034928304919352136,\n",
       "  0.03245939353170494,\n",
       "  0.030483993624026578,\n",
       "  0.028642184644316632,\n",
       "  0.027101927048837144,\n",
       "  0.025615189922973514,\n",
       "  0.024233839785059292,\n",
       "  0.023162481375038624,\n",
       "  0.022057760041207075,\n",
       "  0.02097329613752663,\n",
       "  0.020073426732172567,\n",
       "  0.019233448586116236,\n",
       "  0.01845405778537194],\n",
       " 'loss_td': [2.07314769923687,\n",
       "  1.6540882885456085,\n",
       "  1.3566811333100002,\n",
       "  0.9445571973919868,\n",
       "  0.4427102170884609,\n",
       "  0.20727271803965172,\n",
       "  0.16558238180975118,\n",
       "  0.13608538080006838,\n",
       "  0.09899490885436535,\n",
       "  0.06746575810636084,\n",
       "  0.047486675747980676,\n",
       "  0.03767541563138366,\n",
       "  0.03145097064164778,\n",
       "  0.027484175749123096,\n",
       "  0.024620702490210533,\n",
       "  0.02215144628038009,\n",
       "  0.020275599245602887,\n",
       "  0.01847877927745382,\n",
       "  0.01713234030952056,\n",
       "  0.01591457341176768,\n",
       "  0.014704036021915575,\n",
       "  0.013839638015876213,\n",
       "  0.012895615693802634,\n",
       "  0.012232312428144118,\n",
       "  0.011537720138827959,\n",
       "  0.010838792853367826,\n",
       "  0.010330366087146103,\n",
       "  0.009778811402308444,\n",
       "  0.009270697250030935,\n",
       "  0.008877136744558811],\n",
       " 'acc_si': [1.8663194444444444,\n",
       "  12.803819444444445,\n",
       "  27.604166666666668,\n",
       "  45.57291666666667,\n",
       "  73.39409722222221,\n",
       "  93.96701388888889,\n",
       "  99.17534722222221,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0],\n",
       " 'acc_td': [],\n",
       " 'val-loss_total': [],\n",
       " 'val-loss_si': [],\n",
       " 'val-loss_td': [],\n",
       " 'val-acc_si': [],\n",
       " 'val-acc_td': [],\n",
       " 'LR': [],\n",
       " 'test_acc_si': 99.86979166666666,\n",
       " 'test_acc_td': 96.10558712121212,\n",
       " 'test_f1_si': 0.9925925925925926,\n",
       " 'test_f1_td': 0.959837962962963}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "#         Model\n",
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# list all available torch devices\n",
    "device_list = [\"cpu\"] + [\n",
    "    f\"cuda:{i}\" for i in range(torch.cuda.device_count())\n",
    "]\n",
    "device = device_list[-1] if torch.cuda.is_available() else device_list[0]\n",
    "print(f\"Using device: {device}\")\n",
    "model = LinearLayerShared(\n",
    "    output_size_tasks=NUM_TASKS,\n",
    "    output_size_subjects=NUM_SUBJECTS,\n",
    "    input_size=config[\"d_model_input\"],\n",
    "    intermediate_size=config[\"d_model_intermediate\"],\n",
    "    dropout=config[\"dropout\"],\n",
    ").to(device)\n",
    "wandb_run_name = (\n",
    "    \"LinearShared_best_repro_1000\"\n",
    ")\n",
    "\n",
    "\n",
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "#         training\n",
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config[\"lr\"],\n",
    "    weight_decay=config[\"weight_decay\"],\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=20, gamma=0.1\n",
    ")\n",
    "\n",
    "training_loop(\n",
    "    config[\"epochs\"],\n",
    "    model,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    device,\n",
    "    config,\n",
    "    scheduler=scheduler if config[\"use_scheduler\"] else None,\n",
    "    save_model=False,\n",
    "    save_attention_weights=False,\n",
    "    test_loader=test_loader,\n",
    "    run_name=wandb_run_name,\n",
    "    job_name=\"LinearShared\",\n",
    "    use_deeplift=True,\n",
    "    use_early_stopping=config[\"do_early_stopping\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLbiomed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
