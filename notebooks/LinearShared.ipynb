{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: C:\\Users\\Cyril\\Desktop\\Code\\MIPLab-TeamCEE-DeepLearningforBiomed\\DATA\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd() / \"../code\"))\n",
    "from os import environ\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import logging\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from training import training_loop\n",
    "from utils import balanced_data_shuffle, get_df_raw_data\n",
    "from models import LinearLayerShared\n",
    "## Data path ##\n",
    "DATA_PATH = (Path.cwd().parent / \"DATA\").resolve()\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "DATA_PATH = str(DATA_PATH)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# set deterministic behavior\n",
    "seed = 53498298\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "torch.use_deterministic_algorithms(True)\n",
    "environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # data\n",
    "    \"stratify\": True,\n",
    "    \"validation_split\": 0.2,\n",
    "    # general\n",
    "    \"epochs\": 30,\n",
    "    \"batch_size\": 32,\n",
    "    \"lr\": 1e-4,\n",
    "    \"use_scheduler\": False,\n",
    "    \"do_early_stopping\": False,\n",
    "    \"patience\": 10,\n",
    "    \"best_loss\": 10,\n",
    "    # model\n",
    "    \"d_model_input\": 400,\n",
    "    \"d_model_intermediate\": [1024],\n",
    "    \"d_model_task_output\": 8,\n",
    "    \"d_model_fingerprint_output\": None,  # needs to be determined from data\n",
    "    \"dropout\": 0.1,\n",
    "    \"attention_dropout\": 0.1,\n",
    "    \"num_heads\": 1,\n",
    "    # optimizer\n",
    "    \"lambda_si\": 0.5,\n",
    "    \"lambda_td\": 0.5,\n",
    "    \"weight_decay\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDs = [\n",
    "    100307,\n",
    "    117122,\n",
    "    131722,\n",
    "    153025,\n",
    "    211720,\n",
    "    100408,\n",
    "    118528,\n",
    "    133019,\n",
    "    154734,\n",
    "    212318,\n",
    "    101107,\n",
    "    118730,\n",
    "    133928,\n",
    "    156637,\n",
    "    214423,\n",
    "    101309,\n",
    "    118932,\n",
    "    135225,\n",
    "    159340,\n",
    "    221319,\n",
    "    101915,\n",
    "    120111,\n",
    "    135932,\n",
    "    160123,\n",
    "    239944,\n",
    "    103111,\n",
    "    122317,\n",
    "    136833,\n",
    "    161731,\n",
    "    245333,\n",
    "    103414,\n",
    "    122620,\n",
    "    138534,\n",
    "    162733,\n",
    "    280739,\n",
    "    103818,\n",
    "    123117,\n",
    "    139637,\n",
    "    163129,\n",
    "    298051,\n",
    "    105014,\n",
    "    123925,\n",
    "    140925,\n",
    "    176542,\n",
    "    366446,\n",
    "    105115,\n",
    "    124422,\n",
    "    144832,\n",
    "    178950,\n",
    "    397760,\n",
    "    106016,\n",
    "    125525,\n",
    "    146432,\n",
    "    188347,\n",
    "    414229,\n",
    "    108828,\n",
    "    126325,\n",
    "    147737,\n",
    "    189450,\n",
    "    499566,\n",
    "    110411,\n",
    "    127630,\n",
    "    148335,\n",
    "    190031,\n",
    "    654754,\n",
    "    111312,\n",
    "    127933,\n",
    "    148840,\n",
    "    192540,\n",
    "    672756,\n",
    "    111716,\n",
    "    128127,\n",
    "    149337,\n",
    "    196750,\n",
    "    751348,\n",
    "    113619,\n",
    "    128632,\n",
    "    149539,\n",
    "    198451,\n",
    "    756055,\n",
    "    113922,\n",
    "    129028,\n",
    "    149741,\n",
    "    199655,\n",
    "    792564,\n",
    "    114419,\n",
    "    130013,\n",
    "    151223,\n",
    "    201111,\n",
    "    856766,\n",
    "    115320,\n",
    "    130316,\n",
    "    151526,\n",
    "    208226,\n",
    "    857263,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _show_df_distribution(df):\n",
    "    print(\"Number of samples:\", len(df))\n",
    "    print(\"Unique subjects:\", df[\"subject_id\"].nunique())\n",
    "    print(\"Unique tasks:\", df[\"task\"].nunique())\n",
    "    print(\"*\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining train and test dataframes from all subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subjects: 95\n",
      "Number of tasks: 8\n",
      "Subjects present in train set but not in test set:\n",
      "set()\n",
      "Train set:\n",
      "Number of samples: 608\n",
      "Unique subjects: 95\n",
      "Unique tasks: 8\n",
      "**************************************************\n",
      "Test set:\n",
      "Number of samples: 758\n",
      "Unique subjects: 95\n",
      "Unique tasks: 8\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "# data_dict_train, data_dict_test = get_dict_raw_data(DATA_PATH, IDs[0:3])\n",
    "data_df_train, data_df_test = get_df_raw_data(DATA_PATH, IDs[:])\n",
    "\n",
    "train_dataframe, valid_dataframe = balanced_data_shuffle(\n",
    "    data_df_train,\n",
    "    val_frac=config[\"validation_split\"],\n",
    "    stratify=config[\"stratify\"],\n",
    ")\n",
    "NUM_SUBJECTS = len(data_df_train[\"subject_id\"].unique())\n",
    "print(f\"Number of subjects: {NUM_SUBJECTS}\")\n",
    "NUM_TASKS = data_df_train[\"task\"].nunique()\n",
    "print(f\"Number of tasks: {NUM_TASKS}\")\n",
    "\n",
    "#\n",
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "#         label encoding\n",
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "enc_labels = LabelEncoder()\n",
    "enc_tasks = LabelEncoder()\n",
    "\n",
    "enc_labels.fit(data_df_train[\"subject_id\"].tolist())\n",
    "enc_tasks.fit(data_df_train[\"task\"].tolist())\n",
    "\n",
    "enc_train_label_encodings = enc_labels.transform(\n",
    "    train_dataframe[\"subject_id\"].tolist()\n",
    ")\n",
    "enc_train_task_encodings = enc_tasks.transform(\n",
    "    train_dataframe[\"task\"].tolist()\n",
    ")\n",
    "\n",
    "enc_test_label_encodings = enc_labels.transform(\n",
    "    data_df_test[\"subject_id\"].tolist()\n",
    ")\n",
    "enc_test_task_encodings = enc_tasks.transform(\n",
    "    data_df_test[\"task\"].tolist()\n",
    ")\n",
    "\n",
    "train_dataframe[\"enc_label_id\"] = enc_train_label_encodings\n",
    "train_dataframe[\"enc_task\"] = enc_train_task_encodings\n",
    "data_df_test[\"enc_label_id\"] = enc_test_label_encodings\n",
    "data_df_test[\"enc_task\"] = enc_test_task_encodings\n",
    "\n",
    "print(\"Subjects present in train set but not in test set:\")\n",
    "overlap_set = set(train_dataframe[\"subject_id\"].unique()) - set(\n",
    "    data_df_test[\"subject_id\"].unique()\n",
    ")\n",
    "print(overlap_set)\n",
    "if len(overlap_set) != 0:\n",
    "    print(\"WARNING: subjects present in train set but not in test set\")\n",
    "\n",
    "print(\"Train set:\")\n",
    "_show_df_distribution(train_dataframe)\n",
    "print(\"Test set:\")\n",
    "_show_df_distribution(data_df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjects present in validation set but not in train set:\n",
      "set()\n",
      "Validation set:\n",
      "Number of samples: 152\n",
      "Unique subjects: 76\n",
      "Unique tasks: 8\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(\n",
    "        np.array(train_dataframe[\"mat\"].tolist()).astype(np.float32)\n",
    "    ),\n",
    "    torch.tensor(train_dataframe[\"enc_label_id\"].to_numpy()),\n",
    "    torch.tensor(train_dataframe[\"enc_task\"].to_numpy()),\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=config[\"batch_size\"], shuffle=True\n",
    ")\n",
    "\n",
    "test_dataset = TensorDataset(\n",
    "    torch.tensor(\n",
    "        np.array(data_df_test[\"mat\"].tolist()).astype(np.float32)\n",
    "    ),\n",
    "    torch.tensor(data_df_test[\"enc_label_id\"].to_numpy()),\n",
    "    torch.tensor(data_df_test[\"enc_task\"].to_numpy()),\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=config[\"batch_size\"], shuffle=False\n",
    ")\n",
    "\n",
    "valid_loader = None\n",
    "if valid_dataframe is not None:\n",
    "    enc_valid_label_encodings = enc_labels.transform(\n",
    "        valid_dataframe[\"subject_id\"].tolist()\n",
    "    )\n",
    "    enc_valid_task_encodings = enc_tasks.transform(\n",
    "        valid_dataframe[\"task\"].tolist()\n",
    "    )\n",
    "    valid_dataframe[\"enc_label_id\"] = enc_valid_label_encodings\n",
    "    valid_dataframe[\"enc_task\"] = enc_valid_task_encodings\n",
    "    print(\"Subjects present in validation set but not in train set:\")\n",
    "    overlap_set = set(valid_dataframe[\"subject_id\"].unique()) - set(\n",
    "        train_dataframe[\"subject_id\"].unique()\n",
    "    )\n",
    "    print(overlap_set)\n",
    "    if len(overlap_set) != 0:\n",
    "        print(\n",
    "            \"WARNING: subjects present in validation set but not in train set\"\n",
    "        )\n",
    "    valid_dataset = TensorDataset(\n",
    "        torch.tensor(\n",
    "            np.array(valid_dataframe[\"mat\"].tolist()).astype(np.float32)\n",
    "        ),\n",
    "        torch.tensor(valid_dataframe[\"enc_label_id\"].to_numpy()),\n",
    "        torch.tensor(valid_dataframe[\"enc_task\"].to_numpy()),\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, batch_size=config[\"batch_size\"], shuffle=False\n",
    "    )\n",
    "    print(\"Validation set:\")\n",
    "    _show_df_distribution(valid_dataframe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Using cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mc-achard\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Cyril\\Desktop\\Code\\MIPLab-TeamCEE-DeepLearningforBiomed\\notebooks\\wandb\\run-20231210_120325-xt13cd65</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/c-achard/DLB-Project/runs/xt13cd65' target=\"_blank\">LinearShared_best_repro_W_VAL</a></strong> to <a href='https://wandb.ai/c-achard/DLB-Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/c-achard/DLB-Project' target=\"_blank\">https://wandb.ai/c-achard/DLB-Project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/c-achard/DLB-Project/runs/xt13cd65' target=\"_blank\">https://wandb.ai/c-achard/DLB-Project/runs/xt13cd65</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50 - loss_total: 3.3018 - acc: SI 2.14% / TD 35.03%\n",
      " - val-loss_total: 3.1710 - val-acc: SI 1.88% / TD 60.42% - val-f1: SI 0.0161 / TD 0.5466 - (1.59s/epoch)\n",
      "Epoch: 2/50 - loss_total: 2.9354 - acc: SI 17.76% / TD 76.48%\n",
      " - val-loss_total: 2.9712 - val-acc: SI 2.50% / TD 80.21% - val-f1: SI 0.0232 / TD 0.7546 - (0.96s/epoch)\n",
      "Epoch: 3/50 - loss_total: 2.6332 - acc: SI 30.76% / TD 85.36%\n",
      " - val-loss_total: 2.6970 - val-acc: SI 10.42% / TD 84.79% - val-f1: SI 0.0767 / TD 0.8208 - (0.98s/epoch)\n",
      "Epoch: 4/50 - loss_total: 2.1215 - acc: SI 54.61% / TD 90.62%\n",
      " - val-loss_total: 2.2663 - val-acc: SI 25.00% / TD 83.75% - val-f1: SI 0.1769 / TD 0.8076 - (0.99s/epoch)\n",
      "Epoch: 5/50 - loss_total: 1.3998 - acc: SI 80.92% / TD 95.39%\n",
      " - val-loss_total: 1.9589 - val-acc: SI 45.42% / TD 86.46% - val-f1: SI 0.3792 / TD 0.8421 - (0.98s/epoch)\n",
      "Epoch: 6/50 - loss_total: 0.8155 - acc: SI 97.70% / TD 98.68%\n",
      " - val-loss_total: 1.4635 - val-acc: SI 73.54% / TD 91.25% - val-f1: SI 0.6495 / TD 0.9074 - (0.97s/epoch)\n",
      "Epoch: 7/50 - loss_total: 0.3835 - acc: SI 99.84% / TD 99.67%\n",
      " - val-loss_total: 1.1181 - val-acc: SI 89.17% / TD 92.50% - val-f1: SI 0.8242 / TD 0.9155 - (0.97s/epoch)\n",
      "Epoch: 8/50 - loss_total: 0.1986 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.8511 - val-acc: SI 93.75% / TD 92.50% - val-f1: SI 0.8976 / TD 0.9220 - (0.97s/epoch)\n",
      "Epoch: 9/50 - loss_total: 0.1148 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.7408 - val-acc: SI 96.67% / TD 93.33% - val-f1: SI 0.9429 / TD 0.9259 - (1.14s/epoch)\n",
      "Epoch: 10/50 - loss_total: 0.0717 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.6472 - val-acc: SI 96.67% / TD 92.50% - val-f1: SI 0.9429 / TD 0.9015 - (1.01s/epoch)\n",
      "Epoch: 11/50 - loss_total: 0.0543 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.5882 - val-acc: SI 97.29% / TD 93.12% - val-f1: SI 0.9521 / TD 0.9252 - (0.97s/epoch)\n",
      "Epoch: 12/50 - loss_total: 0.0449 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.5686 - val-acc: SI 98.12% / TD 93.75% - val-f1: SI 0.9695 / TD 0.9126 - (0.96s/epoch)\n",
      "Epoch: 13/50 - loss_total: 0.0394 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.5498 - val-acc: SI 98.75% / TD 93.12% - val-f1: SI 0.9781 / TD 0.9056 - (0.96s/epoch)\n",
      "Epoch: 14/50 - loss_total: 0.0353 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.5373 - val-acc: SI 98.75% / TD 93.12% - val-f1: SI 0.9781 / TD 0.9092 - (0.96s/epoch)\n",
      "Epoch: 15/50 - loss_total: 0.0322 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.5167 - val-acc: SI 98.75% / TD 93.12% - val-f1: SI 0.9781 / TD 0.9071 - (0.95s/epoch)\n",
      "Epoch: 16/50 - loss_total: 0.0297 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.5090 - val-acc: SI 98.75% / TD 93.12% - val-f1: SI 0.9781 / TD 0.9073 - (0.95s/epoch)\n",
      "Epoch: 17/50 - loss_total: 0.0274 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.4956 - val-acc: SI 98.75% / TD 93.12% - val-f1: SI 0.9781 / TD 0.9073 - (0.94s/epoch)\n",
      "Epoch: 18/50 - loss_total: 0.0256 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.4865 - val-acc: SI 98.75% / TD 93.75% - val-f1: SI 0.9781 / TD 0.9126 - (0.95s/epoch)\n",
      "Epoch: 19/50 - loss_total: 0.0239 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.4829 - val-acc: SI 98.12% / TD 93.75% - val-f1: SI 0.9695 / TD 0.9126 - (0.94s/epoch)\n",
      "Epoch: 20/50 - loss_total: 0.0225 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.4702 - val-acc: SI 98.75% / TD 93.12% - val-f1: SI 0.9781 / TD 0.9073 - (0.95s/epoch)\n",
      "Epoch: 21/50 - loss_total: 0.0213 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.4677 - val-acc: SI 98.75% / TD 94.38% - val-f1: SI 0.9781 / TD 0.9192 - (0.95s/epoch)\n",
      "Epoch: 22/50 - loss_total: 0.0201 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.4598 - val-acc: SI 98.75% / TD 93.12% - val-f1: SI 0.9781 / TD 0.9073 - (0.95s/epoch)\n",
      "Epoch: 23/50 - loss_total: 0.0191 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.4532 - val-acc: SI 98.75% / TD 94.38% - val-f1: SI 0.9781 / TD 0.9192 - (0.95s/epoch)\n",
      "Epoch: 24/50 - loss_total: 0.0181 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.4442 - val-acc: SI 98.75% / TD 93.12% - val-f1: SI 0.9781 / TD 0.9073 - (0.94s/epoch)\n",
      "Epoch: 25/50 - loss_total: 0.0172 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.4381 - val-acc: SI 98.75% / TD 93.12% - val-f1: SI 0.9781 / TD 0.9116 - (0.94s/epoch)\n",
      "Epoch: 26/50 - loss_total: 0.0164 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.4371 - val-acc: SI 98.75% / TD 93.75% - val-f1: SI 0.9781 / TD 0.9142 - (0.95s/epoch)\n",
      "Epoch: 27/50 - loss_total: 0.0157 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.4297 - val-acc: SI 98.75% / TD 93.75% - val-f1: SI 0.9781 / TD 0.9142 - (0.94s/epoch)\n",
      "Epoch: 28/50 - loss_total: 0.0151 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.4332 - val-acc: SI 98.75% / TD 93.75% - val-f1: SI 0.9781 / TD 0.9142 - (0.94s/epoch)\n",
      "Epoch: 29/50 - loss_total: 0.0144 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.4250 - val-acc: SI 98.75% / TD 94.38% - val-f1: SI 0.9781 / TD 0.9192 - (0.95s/epoch)\n",
      "Epoch: 30/50 - loss_total: 0.0139 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.4231 - val-acc: SI 98.75% / TD 93.75% - val-f1: SI 0.9781 / TD 0.9142 - (0.95s/epoch)\n",
      "Epoch: 31/50 - loss_total: 0.0133 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.4182 - val-acc: SI 98.75% / TD 94.38% - val-f1: SI 0.9781 / TD 0.9192 - (0.96s/epoch)\n",
      "Epoch: 32/50 - loss_total: 0.0128 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.4151 - val-acc: SI 98.75% / TD 93.75% - val-f1: SI 0.9781 / TD 0.9153 - (0.95s/epoch)\n",
      "Epoch: 33/50 - loss_total: 0.0124 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.4089 - val-acc: SI 98.75% / TD 93.75% - val-f1: SI 0.9781 / TD 0.9153 - (0.95s/epoch)\n",
      "Epoch: 34/50 - loss_total: 0.0119 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.4048 - val-acc: SI 98.75% / TD 93.12% - val-f1: SI 0.9781 / TD 0.9098 - (0.95s/epoch)\n",
      "Epoch: 35/50 - loss_total: 0.0115 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.3996 - val-acc: SI 98.75% / TD 93.75% - val-f1: SI 0.9781 / TD 0.9153 - (0.95s/epoch)\n",
      "Epoch: 36/50 - loss_total: 0.0111 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.4017 - val-acc: SI 98.75% / TD 93.75% - val-f1: SI 0.9781 / TD 0.9140 - (0.95s/epoch)\n",
      "Epoch: 37/50 - loss_total: 0.0107 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.3980 - val-acc: SI 98.75% / TD 93.75% - val-f1: SI 0.9781 / TD 0.9153 - (0.94s/epoch)\n",
      "Epoch: 38/50 - loss_total: 0.0104 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.4020 - val-acc: SI 98.75% / TD 93.75% - val-f1: SI 0.9781 / TD 0.9142 - (0.94s/epoch)\n",
      "Epoch: 39/50 - loss_total: 0.0101 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.3892 - val-acc: SI 98.75% / TD 93.12% - val-f1: SI 0.9781 / TD 0.9114 - (0.95s/epoch)\n",
      "Epoch: 40/50 - loss_total: 0.0097 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.3958 - val-acc: SI 98.75% / TD 94.38% - val-f1: SI 0.9781 / TD 0.9192 - (0.94s/epoch)\n",
      "Epoch: 41/50 - loss_total: 0.0094 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.3912 - val-acc: SI 98.75% / TD 93.75% - val-f1: SI 0.9781 / TD 0.9142 - (0.94s/epoch)\n",
      "Epoch: 42/50 - loss_total: 0.0092 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.3859 - val-acc: SI 98.75% / TD 93.75% - val-f1: SI 0.9781 / TD 0.9153 - (0.94s/epoch)\n",
      "Epoch: 43/50 - loss_total: 0.0089 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.3848 - val-acc: SI 98.75% / TD 93.75% - val-f1: SI 0.9781 / TD 0.9151 - (0.94s/epoch)\n",
      "Epoch: 44/50 - loss_total: 0.0086 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.3832 - val-acc: SI 98.75% / TD 93.12% - val-f1: SI 0.9781 / TD 0.9093 - (0.94s/epoch)\n",
      "Epoch: 45/50 - loss_total: 0.0084 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.3806 - val-acc: SI 98.75% / TD 93.12% - val-f1: SI 0.9781 / TD 0.9093 - (0.99s/epoch)\n",
      "Epoch: 46/50 - loss_total: 0.0082 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.3790 - val-acc: SI 98.75% / TD 93.75% - val-f1: SI 0.9781 / TD 0.9151 - (0.94s/epoch)\n",
      "Epoch: 47/50 - loss_total: 0.0079 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.3794 - val-acc: SI 98.75% / TD 93.12% - val-f1: SI 0.9781 / TD 0.9091 - (0.94s/epoch)\n",
      "Epoch: 48/50 - loss_total: 0.0077 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.3780 - val-acc: SI 98.75% / TD 93.12% - val-f1: SI 0.9781 / TD 0.9093 - (0.94s/epoch)\n",
      "Epoch: 49/50 - loss_total: 0.0075 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.3750 - val-acc: SI 98.75% / TD 93.75% - val-f1: SI 0.9781 / TD 0.9130 - (0.94s/epoch)\n",
      "Epoch: 50/50 - loss_total: 0.0073 - acc: SI 100.00% / TD 100.00%\n",
      " - val-loss_total: 0.3720 - val-acc: SI 98.75% / TD 93.12% - val-f1: SI 0.9781 / TD 0.9093 - (0.94s/epoch)\n",
      "______________________________\n",
      "Final test loss: 0.3257 - acc: SI 99.09% / TD 94.35% - f1: SI 0.9530 / TD 0.9420\n",
      "______________________________\n",
      "Running DeepLIFT\n",
      "SI attributions shape :  torch.Size([32, 400, 400])\n",
      "Running DeepLIFT for task REST1\n",
      "TD attributions shape :  torch.Size([32, 400, 400])\n",
      "Running DeepLIFT for task EMOTION\n",
      "TD attributions shape :  torch.Size([32, 400, 400])\n",
      "Running DeepLIFT for task GAMBLING\n",
      "TD attributions shape :  torch.Size([32, 400, 400])\n",
      "Running DeepLIFT for task LANGUAGE\n",
      "TD attributions shape :  torch.Size([32, 400, 400])\n",
      "Running DeepLIFT for task MOTOR\n",
      "TD attributions shape :  torch.Size([32, 400, 400])\n",
      "Running DeepLIFT for task RELATIONAL\n",
      "TD attributions shape :  torch.Size([32, 400, 400])\n",
      "Running DeepLIFT for task SOCIAL\n",
      "TD attributions shape :  torch.Size([32, 400, 400])\n",
      "Running DeepLIFT for task WM\n",
      "TD attributions shape :  torch.Size([32, 400, 400])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cyril\\anaconda3\\envs\\DLbiomed\\lib\\site-packages\\captum\\_utils\\gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Cyril\\anaconda3\\envs\\DLbiomed\\lib\\site-packages\\captum\\attr\\_core\\deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n",
      "               activations. The hooks and attributes will be removed\n",
      "            after the attribution is finished\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45b3c1eab8c4f3ca47c620531798295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch/Epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>Test/acc_si</td><td>▁</td></tr><tr><td>Test/acc_td</td><td>▁</td></tr><tr><td>Test/f1_si</td><td>▁</td></tr><tr><td>Test/f1_td</td><td>▁</td></tr><tr><td>Test/loss_si</td><td>▁</td></tr><tr><td>Test/loss_td</td><td>▁</td></tr><tr><td>Test/total_loss</td><td>▁</td></tr><tr><td>Train/Epoch-acc_si</td><td>▁▂▃▅████████████████████████████████████</td></tr><tr><td>Train/Epoch-acc_td</td><td>▁▅▆▇████████████████████████████████████</td></tr><tr><td>Train/Epoch-loss_si</td><td>██▇▆▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train/Epoch-loss_td</td><td>█▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train/Epoch-total_loss</td><td>█▇▇▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train/loss_si</td><td>██▇▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train/loss_td</td><td>█▇▅▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train/total_loss</td><td>█▇▆▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Val/Epoch-acc_si</td><td>▁▁▂▃▆▇██████████████████████████████████</td></tr><tr><td>Val/Epoch-acc_td</td><td>▁▅▆▆▇███████████████████████████████████</td></tr><tr><td>Val/Epoch-f1_si</td><td>▁▁▁▂▆▇▇█████████████████████████████████</td></tr><tr><td>Val/Epoch-f1_td</td><td>▁▅▆▆████████████████████████████████████</td></tr><tr><td>Val/Epoch-loss_si</td><td>███▇▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Val/Epoch-loss_td</td><td>█▇▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Val/Epoch-total_loss</td><td>██▇▆▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch/Epoch</td><td>50</td></tr><tr><td>Test/acc_si</td><td>99.08854</td></tr><tr><td>Test/acc_td</td><td>94.35369</td></tr><tr><td>Test/f1_si</td><td>0.95296</td></tr><tr><td>Test/f1_td</td><td>0.94203</td></tr><tr><td>Test/loss_si</td><td>0.46356</td></tr><tr><td>Test/loss_td</td><td>0.18779</td></tr><tr><td>Test/total_loss</td><td>0.32568</td></tr><tr><td>Train/Epoch-acc_si</td><td>100.0</td></tr><tr><td>Train/Epoch-acc_td</td><td>100.0</td></tr><tr><td>Train/Epoch-loss_si</td><td>0.01018</td></tr><tr><td>Train/Epoch-loss_td</td><td>0.00446</td></tr><tr><td>Train/Epoch-total_loss</td><td>0.00732</td></tr><tr><td>Train/loss_si</td><td>0.01124</td></tr><tr><td>Train/loss_td</td><td>0.00493</td></tr><tr><td>Train/total_loss</td><td>0.00809</td></tr><tr><td>Val/Epoch-acc_si</td><td>98.75</td></tr><tr><td>Val/Epoch-acc_td</td><td>93.125</td></tr><tr><td>Val/Epoch-f1_si</td><td>0.97806</td></tr><tr><td>Val/Epoch-f1_td</td><td>0.90934</td></tr><tr><td>Val/Epoch-loss_si</td><td>0.47682</td></tr><tr><td>Val/Epoch-loss_td</td><td>0.26727</td></tr><tr><td>Val/Epoch-total_loss</td><td>0.37205</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LinearShared_best_repro_W_VAL</strong> at: <a href='https://wandb.ai/c-achard/DLB-Project/runs/xt13cd65' target=\"_blank\">https://wandb.ai/c-achard/DLB-Project/runs/xt13cd65</a><br/>Synced 6 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231210_120325-xt13cd65\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'epoch': 50,\n",
       " 'loss_total': [3.30184080726222,\n",
       "  2.935366618005853,\n",
       "  2.633178761130885,\n",
       "  2.121483878085488,\n",
       "  1.399751901626587,\n",
       "  0.8154524470630445,\n",
       "  0.3835297399445584,\n",
       "  0.19859100486102857,\n",
       "  0.11484789377764652,\n",
       "  0.07171060105687693,\n",
       "  0.054259371012449265,\n",
       "  0.04486438141841637,\n",
       "  0.03937306804092307,\n",
       "  0.03533292679410232,\n",
       "  0.03218385930124082,\n",
       "  0.02965838530738103,\n",
       "  0.027402092163500032,\n",
       "  0.025566780057392623,\n",
       "  0.023889921997722826,\n",
       "  0.022500808105656977,\n",
       "  0.021258727892449026,\n",
       "  0.020121032274083087,\n",
       "  0.01906267132021879,\n",
       "  0.01811728695113408,\n",
       "  0.017216212361266737,\n",
       "  0.01642622102640177,\n",
       "  0.01571830523837554,\n",
       "  0.015060148299916795,\n",
       "  0.014444554371661261,\n",
       "  0.013857873352734666,\n",
       "  0.013302491683708994,\n",
       "  0.012808495348221377,\n",
       "  0.012351369730343944,\n",
       "  0.01190065112161009,\n",
       "  0.011490144080629474,\n",
       "  0.011116717304838332,\n",
       "  0.010740474258598528,\n",
       "  0.010386343310145955,\n",
       "  0.010058236700531683,\n",
       "  0.00974701104783698,\n",
       "  0.00944216917023847,\n",
       "  0.00915094009159427,\n",
       "  0.00887736864387989,\n",
       "  0.008631292681552862,\n",
       "  0.00839491340478784,\n",
       "  0.008153983587889295,\n",
       "  0.00792317577686749,\n",
       "  0.007720218456693385,\n",
       "  0.007512076145136042,\n",
       "  0.007316731382161379],\n",
       " 'loss_si': [4.660860212225663,\n",
       "  4.344465732574463,\n",
       "  4.124482995585391,\n",
       "  3.5943080877002918,\n",
       "  2.5209104136416784,\n",
       "  1.4476440643009387,\n",
       "  0.635189939486353,\n",
       "  0.2881032189256267,\n",
       "  0.14819306762594925,\n",
       "  0.09007775352189415,\n",
       "  0.06818093555538278,\n",
       "  0.0565984807908535,\n",
       "  0.050022046816976445,\n",
       "  0.04530972791345496,\n",
       "  0.04151084489728275,\n",
       "  0.0383879091394575,\n",
       "  0.03558561362718281,\n",
       "  0.0333857058890556,\n",
       "  0.031287813069004765,\n",
       "  0.029541055328751866,\n",
       "  0.027979422556726558,\n",
       "  0.026558728986664823,\n",
       "  0.025264598037067213,\n",
       "  0.024097591148395287,\n",
       "  0.022903902162062496,\n",
       "  0.02192725888208339,\n",
       "  0.021033530956820437,\n",
       "  0.02024307052947973,\n",
       "  0.019430391392425486,\n",
       "  0.018687962995547997,\n",
       "  0.0179489272597589,\n",
       "  0.017314922554712547,\n",
       "  0.016715542658379202,\n",
       "  0.016174026960997206,\n",
       "  0.015631480926745815,\n",
       "  0.015133778368564029,\n",
       "  0.014675061365491465,\n",
       "  0.014206723310053349,\n",
       "  0.013777298852801323,\n",
       "  0.013359602197612586,\n",
       "  0.01299765622733455,\n",
       "  0.012597707688416305,\n",
       "  0.01223746652861959,\n",
       "  0.011904276613342134,\n",
       "  0.011603429913520813,\n",
       "  0.011294478158417501,\n",
       "  0.010991225656318037,\n",
       "  0.010700543656160957,\n",
       "  0.010438979015146432,\n",
       "  0.010176618240381541],\n",
       " 'loss_td': [1.9428214022987766,\n",
       "  1.5262675159855892,\n",
       "  1.141874539224725,\n",
       "  0.6486596778819436,\n",
       "  0.27859340059129817,\n",
       "  0.18326083060942197,\n",
       "  0.13186954550052943,\n",
       "  0.10907878687507228,\n",
       "  0.0815027185568684,\n",
       "  0.05334344800365599,\n",
       "  0.04033780686165157,\n",
       "  0.033130281261707605,\n",
       "  0.028724089853073422,\n",
       "  0.025356126164919453,\n",
       "  0.02285687497964031,\n",
       "  0.020928861671372465,\n",
       "  0.01921857099391912,\n",
       "  0.017747854715899417,\n",
       "  0.01649203117152578,\n",
       "  0.01546056039239231,\n",
       "  0.014538033326205454,\n",
       "  0.013683335757569262,\n",
       "  0.012860744995506187,\n",
       "  0.012136983096991716,\n",
       "  0.011528522511454005,\n",
       "  0.01092518341580504,\n",
       "  0.010403079372879705,\n",
       "  0.009877225923302927,\n",
       "  0.009458717399914014,\n",
       "  0.009027783709921335,\n",
       "  0.008656056205693045,\n",
       "  0.008302067970170787,\n",
       "  0.007987196704274729,\n",
       "  0.007627275600833328,\n",
       "  0.007348807332547088,\n",
       "  0.0070996562656211225,\n",
       "  0.006805887053671636,\n",
       "  0.0065659633102385625,\n",
       "  0.006339174670804488,\n",
       "  0.006134420069620798,\n",
       "  0.005886681917074479,\n",
       "  0.005704172519280722,\n",
       "  0.005517270587580769,\n",
       "  0.005358308725255101,\n",
       "  0.005186396969580336,\n",
       "  0.0050134888948186445,\n",
       "  0.004855125872908454,\n",
       "  0.004739893110174882,\n",
       "  0.004585173226108677,\n",
       "  0.004456844695500638],\n",
       " 'acc_si': [2.138157894736842,\n",
       "  17.763157894736842,\n",
       "  30.756578947368425,\n",
       "  54.60526315789473,\n",
       "  80.92105263157895,\n",
       "  97.69736842105263,\n",
       "  99.83552631578947,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0,\n",
       "  100.0],\n",
       " 'acc_td': [],\n",
       " 'val-loss_total': [3.1710053443908692,\n",
       "  2.971227502822876,\n",
       "  2.6969934463500977,\n",
       "  2.266327714920044,\n",
       "  1.9589190006256103,\n",
       "  1.4634621858596801,\n",
       "  1.1180852890014648,\n",
       "  0.8511459589004516,\n",
       "  0.7408474206924438,\n",
       "  0.647157895565033,\n",
       "  0.5881890296936035,\n",
       "  0.5686121165752411,\n",
       "  0.549769401550293,\n",
       "  0.5373107492923737,\n",
       "  0.5166520297527313,\n",
       "  0.509029644727707,\n",
       "  0.4956373512744904,\n",
       "  0.4865222632884979,\n",
       "  0.4828687846660614,\n",
       "  0.4702398180961609,\n",
       "  0.4676564812660217,\n",
       "  0.45979578495025636,\n",
       "  0.45322837829589846,\n",
       "  0.44424635767936704,\n",
       "  0.43807925581932067,\n",
       "  0.43707195520401,\n",
       "  0.4296589016914368,\n",
       "  0.43323538899421693,\n",
       "  0.42504643797874453,\n",
       "  0.42314141392707827,\n",
       "  0.4181908845901489,\n",
       "  0.4151312470436096,\n",
       "  0.40889578461647036,\n",
       "  0.4048420190811157,\n",
       "  0.3995601534843445,\n",
       "  0.401695317029953,\n",
       "  0.3980222404003143,\n",
       "  0.4020149052143097,\n",
       "  0.38917062282562254,\n",
       "  0.39579410552978517,\n",
       "  0.3912447154521942,\n",
       "  0.38592604398727415,\n",
       "  0.3847854912281036,\n",
       "  0.3831774592399597,\n",
       "  0.38063769936561587,\n",
       "  0.379019170999527,\n",
       "  0.3793570756912231,\n",
       "  0.37797672152519224,\n",
       "  0.37499955892562864,\n",
       "  0.3720491647720337],\n",
       " 'val-loss_si': [4.646674251556396,\n",
       "  4.548505878448486,\n",
       "  4.387145709991455,\n",
       "  3.9366227626800536,\n",
       "  3.39591498374939,\n",
       "  2.5463213443756105,\n",
       "  1.8810267210006715,\n",
       "  1.370846199989319,\n",
       "  1.1630730628967285,\n",
       "  0.991221034526825,\n",
       "  0.8902510046958924,\n",
       "  0.8529807448387146,\n",
       "  0.8236052513122558,\n",
       "  0.7944398760795593,\n",
       "  0.7605663537979126,\n",
       "  0.7452619671821594,\n",
       "  0.7268238663673401,\n",
       "  0.7060244917869568,\n",
       "  0.6953890800476075,\n",
       "  0.674063217639923,\n",
       "  0.6633818626403809,\n",
       "  0.6520760655403137,\n",
       "  0.6367148876190185,\n",
       "  0.6219048976898194,\n",
       "  0.6156704783439636,\n",
       "  0.6090462565422058,\n",
       "  0.5947402477264404,\n",
       "  0.5961967825889587,\n",
       "  0.5865200161933899,\n",
       "  0.5812151193618774,\n",
       "  0.5708317518234253,\n",
       "  0.563859498500824,\n",
       "  0.5538276553153991,\n",
       "  0.5460166931152344,\n",
       "  0.5382112860679626,\n",
       "  0.5388113260269165,\n",
       "  0.5312949478626251,\n",
       "  0.530049329996109,\n",
       "  0.5196133255958557,\n",
       "  0.5199571311473846,\n",
       "  0.513523280620575,\n",
       "  0.5070908725261688,\n",
       "  0.5052280604839325,\n",
       "  0.5004033923149109,\n",
       "  0.49528645277023314,\n",
       "  0.49093787670135497,\n",
       "  0.49139750599861143,\n",
       "  0.4846103310585022,\n",
       "  0.48062925934791567,\n",
       "  0.4768243312835693],\n",
       " 'val-loss_td': [1.6953364610671997,\n",
       "  1.3939491271972657,\n",
       "  1.0068411469459533,\n",
       "  0.5960327208042144,\n",
       "  0.5219230353832245,\n",
       "  0.3806030094623566,\n",
       "  0.35514389276504515,\n",
       "  0.3314457267522812,\n",
       "  0.31862176954746246,\n",
       "  0.3030947774648666,\n",
       "  0.2861270561814308,\n",
       "  0.2842435047030449,\n",
       "  0.2759335398674011,\n",
       "  0.28018161952495574,\n",
       "  0.2727377384901047,\n",
       "  0.2727973431348801,\n",
       "  0.26445085257291795,\n",
       "  0.2670200377702713,\n",
       "  0.27034847885370256,\n",
       "  0.2664164170622826,\n",
       "  0.2719310909509659,\n",
       "  0.2675155088305473,\n",
       "  0.26974185556173325,\n",
       "  0.2665878117084503,\n",
       "  0.2604880228638649,\n",
       "  0.2650976523756981,\n",
       "  0.2645775780081749,\n",
       "  0.2702740103006363,\n",
       "  0.2635728642344475,\n",
       "  0.26506769806146624,\n",
       "  0.26555000692605973,\n",
       "  0.2664029970765114,\n",
       "  0.26396391838788985,\n",
       "  0.2636673480272293,\n",
       "  0.26090901866555216,\n",
       "  0.2645793162286282,\n",
       "  0.2647495374083519,\n",
       "  0.2739804729819298,\n",
       "  0.2587279289960861,\n",
       "  0.2716310888528824,\n",
       "  0.26896613985300066,\n",
       "  0.2647612303495407,\n",
       "  0.2643429130315781,\n",
       "  0.2659515216946602,\n",
       "  0.26598893478512764,\n",
       "  0.26710047870874404,\n",
       "  0.26731663942337036,\n",
       "  0.27134311348199847,\n",
       "  0.2693698428571224,\n",
       "  0.2672740012407303],\n",
       " 'val-acc_si': [1.875,\n",
       "  2.5,\n",
       "  10.416666666666668,\n",
       "  25.0,\n",
       "  45.41666666666667,\n",
       "  73.54166666666667,\n",
       "  89.16666666666666,\n",
       "  93.75,\n",
       "  96.66666666666666,\n",
       "  96.66666666666666,\n",
       "  97.29166666666667,\n",
       "  98.125,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.125,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.75,\n",
       "  98.75],\n",
       " 'val-acc_td': [60.41666666666667,\n",
       "  80.20833333333334,\n",
       "  84.79166666666667,\n",
       "  83.75,\n",
       "  86.45833333333334,\n",
       "  91.25,\n",
       "  92.5,\n",
       "  92.5,\n",
       "  93.33333333333333,\n",
       "  92.5,\n",
       "  93.125,\n",
       "  93.75,\n",
       "  93.125,\n",
       "  93.125,\n",
       "  93.125,\n",
       "  93.125,\n",
       "  93.125,\n",
       "  93.75,\n",
       "  93.75,\n",
       "  93.125,\n",
       "  94.375,\n",
       "  93.125,\n",
       "  94.375,\n",
       "  93.125,\n",
       "  93.125,\n",
       "  93.75,\n",
       "  93.75,\n",
       "  93.75,\n",
       "  94.375,\n",
       "  93.75,\n",
       "  94.375,\n",
       "  93.75,\n",
       "  93.75,\n",
       "  93.125,\n",
       "  93.75,\n",
       "  93.75,\n",
       "  93.75,\n",
       "  93.75,\n",
       "  93.125,\n",
       "  94.375,\n",
       "  93.75,\n",
       "  93.75,\n",
       "  93.75,\n",
       "  93.125,\n",
       "  93.125,\n",
       "  93.75,\n",
       "  93.125,\n",
       "  93.125,\n",
       "  93.75,\n",
       "  93.125],\n",
       " 'LR': [0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001,\n",
       "  0.0001],\n",
       " 'test_acc_si': 99.08854166666666,\n",
       " 'test_acc_td': 94.35369318181817,\n",
       " 'test_f1_si': 0.9529570973688622,\n",
       " 'test_f1_td': 0.942034556878307}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "#         Model\n",
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# list all available torch devices\n",
    "device_list = [\"cpu\"] + [\n",
    "    f\"cuda:{i}\" for i in range(torch.cuda.device_count())\n",
    "]\n",
    "device = device_list[-1] if torch.cuda.is_available() else device_list[0]\n",
    "print(f\"Using device: {device}\")\n",
    "model = LinearLayerShared(\n",
    "    output_size_tasks=NUM_TASKS,\n",
    "    output_size_subjects=NUM_SUBJECTS,\n",
    "    input_size=config[\"d_model_input\"],\n",
    "    intermediate_size=config[\"d_model_intermediate\"],\n",
    "    dropout=config[\"dropout\"],\n",
    ").to(device)\n",
    "wandb_run_name = (\n",
    "    \"LinearShared_best_repro_W_VAL\"\n",
    ")\n",
    "\n",
    "\n",
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "#         training\n",
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config[\"lr\"],\n",
    "    weight_decay=config[\"weight_decay\"],\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=20, gamma=0.1\n",
    ")\n",
    "\n",
    "training_loop(\n",
    "    config[\"epochs\"],\n",
    "    model,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    device,\n",
    "    config,\n",
    "    scheduler=scheduler if config[\"use_scheduler\"] else None,\n",
    "    save_model=False,\n",
    "    save_attention_weights=False,\n",
    "    test_loader=test_loader,\n",
    "    run_name=wandb_run_name,\n",
    "    job_name=\"LinearShared\",\n",
    "    use_deeplift=True,\n",
    "    use_early_stopping=config[\"do_early_stopping\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLbiomed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
