{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:git.cmd:Popen(['git', 'version'], cwd=c:\\Users\\Cyril\\Desktop\\Code\\MIPLab-TeamCEE-DeepLearningforBiomed\\notebooks, universal_newlines=False, shell=None, istream=None)\n",
      "DEBUG:git.cmd:Popen(['git', 'version'], cwd=c:\\Users\\Cyril\\Desktop\\Code\\MIPLab-TeamCEE-DeepLearningforBiomed\\notebooks, universal_newlines=False, shell=None, istream=None)\n",
      "DEBUG:wandb.docker.auth:Trying paths: ['C:\\\\Users\\\\Cyril\\\\.docker\\\\config.json', 'C:\\\\Users\\\\Cyril\\\\.dockercfg']\n",
      "DEBUG:wandb.docker.auth:Found file at path: C:\\Users\\Cyril\\.docker\\config.json\n",
      "DEBUG:wandb.docker.auth:Found 'auths' section\n",
      "DEBUG:wandb.docker.auth:Auth data for https://index.docker.io/v1/ is absent. Client might be using a credentials store instead.\n",
      "DEBUG:wandb.docker.auth:Found 'credsStore' section\n",
      "DEBUG:sentry_sdk.errors:[Tracing] Create new propagation context: {'trace_id': 'bec9f8f2c76c453bb25cc742939ebabe', 'span_id': 'b35d6d8b3d4c5dc8', 'parent_span_id': None, 'dynamic_sampling_context': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: C:\\Users\\Cyril\\Desktop\\Code\\MIPLab-TeamCEE-DeepLearningforBiomed\\DATA\n"
     ]
    }
   ],
   "source": [
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "#         imports\n",
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "import sys\n",
    "sys.path.append('../code/')\n",
    "from training import *\n",
    "from models import *\n",
    "from utils import * \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from pathlib import Path\n",
    "\n",
    "## Data path ##\n",
    "DATA_PATH = (Path.cwd().parent / \"DATA\").resolve() # TODO : adapt to server\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "DATA_PATH = str(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "#         hyperparameters\n",
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "config = {\n",
    "    # general\n",
    "    \"epochs\": 100,\n",
    "    \"batch_size\": 4,\n",
    "    \"lr\": 1e-3,\n",
    "\n",
    "    # model\n",
    "    \"d_model_input\": 400,\n",
    "    \"d_model_intermediate\": 512,\n",
    "    \"d_model_task_output\": 8,\n",
    "    \"d_model_fingerprint_output\": None, # needs to be determined from data\n",
    "    \"dropout\" : 0.1,\n",
    "    \"attention_dropout\" : 0.1,\n",
    "    \"num_heads\": 4,\n",
    "    \"num_layers\": 0, # TBA?\n",
    "\n",
    "    # optimizer\n",
    "    \"lambda_si\": 0.5,\n",
    "    \"lambda_td\": 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "#         subject ID list\n",
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "IDs = [100307,  117122,  131722,  153025,  211720,\n",
    "100408,  118528,  133019,  154734,  212318,      \n",
    "101107,  118730,  133928,  156637,  214423,        \n",
    "101309,  118932,  135225,  159340,  221319,       \n",
    "101915,  120111,  135932,  160123,  239944 ,      \n",
    "103111,  122317,  136833,  161731,  245333,        \n",
    "103414,  122620,  138534,  162733,  280739,        \n",
    "103818, 123117,  139637,  163129,  298051,        \n",
    "105014,  123925,  140925,  176542,  366446,        \n",
    "105115,  124422,  144832,  178950,  397760,        \n",
    "106016,  125525,  146432,  188347,  414229,        \n",
    "108828,  126325,  147737,  189450,  499566,\n",
    "110411,  127630,  148335,  190031,  654754,\n",
    "111312,  127933,  148840,  192540,  672756,\n",
    "111716,  128127,  149337,  196750,  751348,\n",
    "113619,  128632,  149539,  198451,  756055,\n",
    "113922,  129028,  149741,  199655,  792564,\n",
    "114419,  130013,  151223,  201111,  856766,\n",
    "115320,  130316,  151526,  208226,  857263]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>task_id</th>\n",
       "      <th>mat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100307</td>\n",
       "      <td>REST1</td>\n",
       "      <td>[[0.21854491103466994, 0.07509374392964863, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100307</td>\n",
       "      <td>REST2</td>\n",
       "      <td>[[0.2509722712619662, 0.06429771271159306, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100307</td>\n",
       "      <td>EMOTION</td>\n",
       "      <td>[[0.27626702525883573, 0.03827488524289221, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100307</td>\n",
       "      <td>GAMBLING</td>\n",
       "      <td>[[0.2356709594115424, 0.03883497545044236, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100307</td>\n",
       "      <td>LANGUAGE</td>\n",
       "      <td>[[0.2317390561241142, 0.06537822245634475, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100307</td>\n",
       "      <td>MOTOR</td>\n",
       "      <td>[[0.2141270371266362, 0.040754342863046, 0.084...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100307</td>\n",
       "      <td>RELATIONAL</td>\n",
       "      <td>[[0.2709434948110919, 0.08915439190003989, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100307</td>\n",
       "      <td>SOCIAL</td>\n",
       "      <td>[[0.27075755129825896, 0.07942572217389814, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100307</td>\n",
       "      <td>WM</td>\n",
       "      <td>[[0.28122430896568573, 0.12358947079320645, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100408</td>\n",
       "      <td>REST1</td>\n",
       "      <td>[[0.31926332845816463, 0.18973934066088882, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject_id     task_id                                                mat\n",
       "0     100307       REST1  [[0.21854491103466994, 0.07509374392964863, 0....\n",
       "1     100307       REST2  [[0.2509722712619662, 0.06429771271159306, 0.1...\n",
       "2     100307     EMOTION  [[0.27626702525883573, 0.03827488524289221, 0....\n",
       "3     100307    GAMBLING  [[0.2356709594115424, 0.03883497545044236, 0.1...\n",
       "4     100307    LANGUAGE  [[0.2317390561241142, 0.06537822245634475, 0.0...\n",
       "5     100307       MOTOR  [[0.2141270371266362, 0.040754342863046, 0.084...\n",
       "6     100307  RELATIONAL  [[0.2709434948110919, 0.08915439190003989, 0.1...\n",
       "7     100307      SOCIAL  [[0.27075755129825896, 0.07942572217389814, 0....\n",
       "8     100307          WM  [[0.28122430896568573, 0.12358947079320645, 0....\n",
       "9     100408       REST1  [[0.31926332845816463, 0.18973934066088882, 0...."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "#         joining train and test dataframes from all subjects\n",
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# data_dict_train, data_dict_test = get_dict_raw_data(DATA_PATH, IDs[0:3])\n",
    "data_df_train, data_df_test = get_df_raw_data(DATA_PATH, [IDs[0], IDs[5], IDs[10]])\n",
    "display(data_df_train.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subjects: 3\n"
     ]
    }
   ],
   "source": [
    "NUM_SUBJECTS = len(data_df_train[\"subject_id\"].unique())\n",
    "print(f\"Number of subjects: {NUM_SUBJECTS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "#         label encoding\n",
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# one hot encoding\n",
    "\n",
    "# enc_labels = OneHotEncoder(handle_unknown='ignore')\n",
    "# enc_tasks = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# enc_labels.fit(data_dict_train[\"subject_id\"].to_numpy().reshape(-1, 1))\n",
    "# enc_tasks.fit(data_dict_train[\"task_id\"].to_numpy().reshape(-1, 1))\n",
    "\n",
    "# enc_train_label_encodings = enc_labels.transform(data_dict_train[\"subject_id\"].to_numpy().reshape(-1, 1)).toarray()\n",
    "# enc_train_task_encodings = enc_tasks.transform(data_dict_train[\"task_id\"].to_numpy().reshape(-1, 1)).toarray()\n",
    "\n",
    "# enc_test_label_encodings = enc_labels.transform(data_dict_test[\"subject_id\"].to_numpy().reshape(-1, 1)).toarray()\n",
    "# enc_test_task_encodings = enc_tasks.transform(data_dict_test[\"task_id\"].to_numpy().reshape(-1, 1)).toarray()\n",
    "\n",
    "# data_dict_train[\"enc_label_id\"] = enc_train_label_encodings.tolist()\n",
    "# data_dict_train[\"enc_task_id\"] = enc_train_task_encodings.tolist()\n",
    "\n",
    "# data_dict_test[\"enc_label_id\"] = enc_test_label_encodings.tolist()\n",
    "# data_dict_test[\"enc_task_id\"] = enc_test_task_encodings.tolist()\n",
    "\n",
    "# label encoding\n",
    "enc_labels = LabelEncoder()\n",
    "enc_tasks = LabelEncoder()\n",
    "\n",
    "enc_labels.fit(data_df_train[\"subject_id\"].tolist())\n",
    "enc_tasks.fit(data_df_train[\"task_id\"].tolist())\n",
    "\n",
    "enc_train_label_encodings = enc_labels.transform(data_df_train[\"subject_id\"].tolist())\n",
    "enc_train_task_encodings = enc_tasks.transform(data_df_train[\"task_id\"].tolist())\n",
    "\n",
    "enc_test_label_encodings = enc_labels.transform(data_df_test[\"subject_id\"].tolist())\n",
    "enc_test_task_encodings = enc_tasks.transform(data_df_test[\"task_id\"].tolist())\n",
    "\n",
    "data_df_train[\"enc_label_id\"] = enc_train_label_encodings\n",
    "data_df_train[\"enc_task_id\"] = enc_train_task_encodings\n",
    "data_df_test[\"enc_label_id\"] = enc_test_label_encodings\n",
    "data_df_test[\"enc_task_id\"] = enc_test_task_encodings\n",
    "\n",
    "#enc.inverse_transform() to reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>task_id</th>\n",
       "      <th>mat</th>\n",
       "      <th>enc_label_id</th>\n",
       "      <th>enc_task_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100307</td>\n",
       "      <td>REST1</td>\n",
       "      <td>[[0.21854491103466994, 0.07509374392964863, 0....</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100307</td>\n",
       "      <td>REST2</td>\n",
       "      <td>[[0.2509722712619662, 0.06429771271159306, 0.1...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100307</td>\n",
       "      <td>EMOTION</td>\n",
       "      <td>[[0.27626702525883573, 0.03827488524289221, 0....</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100307</td>\n",
       "      <td>GAMBLING</td>\n",
       "      <td>[[0.2356709594115424, 0.03883497545044236, 0.1...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100307</td>\n",
       "      <td>LANGUAGE</td>\n",
       "      <td>[[0.2317390561241142, 0.06537822245634475, 0.0...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100307</td>\n",
       "      <td>MOTOR</td>\n",
       "      <td>[[0.2141270371266362, 0.040754342863046, 0.084...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100307</td>\n",
       "      <td>RELATIONAL</td>\n",
       "      <td>[[0.2709434948110919, 0.08915439190003989, 0.1...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100307</td>\n",
       "      <td>SOCIAL</td>\n",
       "      <td>[[0.27075755129825896, 0.07942572217389814, 0....</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100307</td>\n",
       "      <td>WM</td>\n",
       "      <td>[[0.28122430896568573, 0.12358947079320645, 0....</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100408</td>\n",
       "      <td>REST1</td>\n",
       "      <td>[[0.31926332845816463, 0.18973934066088882, 0....</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject_id     task_id                                                mat  \\\n",
       "0     100307       REST1  [[0.21854491103466994, 0.07509374392964863, 0....   \n",
       "1     100307       REST2  [[0.2509722712619662, 0.06429771271159306, 0.1...   \n",
       "2     100307     EMOTION  [[0.27626702525883573, 0.03827488524289221, 0....   \n",
       "3     100307    GAMBLING  [[0.2356709594115424, 0.03883497545044236, 0.1...   \n",
       "4     100307    LANGUAGE  [[0.2317390561241142, 0.06537822245634475, 0.0...   \n",
       "5     100307       MOTOR  [[0.2141270371266362, 0.040754342863046, 0.084...   \n",
       "6     100307  RELATIONAL  [[0.2709434948110919, 0.08915439190003989, 0.1...   \n",
       "7     100307      SOCIAL  [[0.27075755129825896, 0.07942572217389814, 0....   \n",
       "8     100307          WM  [[0.28122430896568573, 0.12358947079320645, 0....   \n",
       "9     100408       REST1  [[0.31926332845816463, 0.18973934066088882, 0....   \n",
       "\n",
       "   enc_label_id  enc_task_id  \n",
       "0             0            5  \n",
       "1             0            6  \n",
       "2             0            0  \n",
       "3             0            1  \n",
       "4             0            2  \n",
       "5             0            3  \n",
       "6             0            4  \n",
       "7             0            7  \n",
       "8             0            8  \n",
       "9             1            5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data_df_train.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "#         initializing dataloader objects\n",
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(\n",
    "        np.array(data_df_train[\"mat\"].tolist()).astype(np.float32)\n",
    "    ),\n",
    "    torch.tensor(data_df_train[\"enc_label_id\"].to_numpy()),\n",
    "    torch.tensor(data_df_train[\"enc_task_id\"].to_numpy()))\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(\n",
    "    torch.tensor(\n",
    "        np.array(data_df_test[\"mat\"].tolist()).astype(np.float32)\n",
    "    ),\n",
    "    torch.tensor(data_df_test[\"enc_label_id\"].to_numpy()),\n",
    "    torch.tensor(data_df_test[\"enc_task_id\"].to_numpy()))\n",
    "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n",
      "torch.Size([1, 9])\n",
      "torch.Size([1, 400, 400])\n"
     ]
    }
   ],
   "source": [
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "#         initializing model\n",
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "model = MRIVisionTransformers(\n",
    "        # output_size_tasks = config[\"d_model_task_output\"],\n",
    "        output_size_tasks = 9,\n",
    "        output_size_subjects = NUM_SUBJECTS,\n",
    "        input_size = config[\"d_model_input\"],\n",
    "        num_heads = config[\"num_heads\"],\n",
    "        dropout = config[\"dropout\"],\n",
    "        attention_dropout = config[\"attention_dropout\"]\n",
    ")\n",
    "\n",
    "x = torch.randn(1, 400, 400)\n",
    "y = model(x)\n",
    "\n",
    "# x_si, x_td, attn_weights\n",
    "print(y[0].size())\n",
    "print(y[1].size())\n",
    "print(y[2].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:SI loss : 25.16659697464534\n",
      "INFO:training:TD loss : 18.557863235473633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 - loss_total: 22.5231- Acc: SI 0.6667 / TD 0.3333- val-loss_total: 42.0800 - val-acc: SI 1.0000 TD 0.0000(12.02s/epoch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:SI loss : 16.577828918184554\n",
      "INFO:training:TD loss : 21.250197955540248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/100 - loss_total: 18.4468- Acc: SI 0.3333 / TD 0.0000- val-loss_total: 6.8517 - val-acc: SI 1.0000 TD 0.0000(12.96s/epoch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:SI loss : 7.796231324119227\n",
      "INFO:training:TD loss : 9.019089460372925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/100 - loss_total: 8.2854- Acc: SI 0.0000 / TD 0.0000- val-loss_total: 6.3553 - val-acc: SI 0.3333 TD 0.3333(13.72s/epoch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:SI loss : 1.7898836902209692\n",
      "INFO:training:TD loss : 4.078743491853986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/100 - loss_total: 2.7054- Acc: SI 1.0000 / TD 0.6667- val-loss_total: 4.4765 - val-acc: SI 0.0000 TD 0.0000(11.06s/epoch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:SI loss : 1.654969836984362\n",
      "INFO:training:TD loss : 3.028984018734523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/100 - loss_total: 2.2046- Acc: SI 0.3333 / TD 0.3333- val-loss_total: 3.9961 - val-acc: SI 0.3333 TD 0.0000(11.28s/epoch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:SI loss : 0.3685093961123909\n",
      "INFO:training:TD loss : 2.2389408371278217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/100 - loss_total: 1.1167- Acc: SI 1.0000 / TD 0.6667- val-loss_total: 2.9472 - val-acc: SI 0.6667 TD 0.0000(11.10s/epoch)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training:SI loss : 0.1294171858046736\n",
      "INFO:training:TD loss : 1.1089805965977055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/100 - loss_total: 0.5212- Acc: SI 0.6667 / TD 1.0000- val-loss_total: 2.9397 - val-acc: SI 0.0000 TD 0.0000(11.39s/epoch)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Cyril\\Desktop\\Code\\MIPLab-TeamCEE-DeepLearningforBiomed\\notebooks\\run.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Cyril/Desktop/Code/MIPLab-TeamCEE-DeepLearningforBiomed/notebooks/run.ipynb#X40sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# change to cuda\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Cyril/Desktop/Code/MIPLab-TeamCEE-DeepLearningforBiomed/notebooks/run.ipynb#X40sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Cyril/Desktop/Code/MIPLab-TeamCEE-DeepLearningforBiomed/notebooks/run.ipynb#X40sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m training_loop(config[\u001b[39m\"\u001b[39;49m\u001b[39mepochs\u001b[39;49m\u001b[39m\"\u001b[39;49m], model, train_loader, test_loader, criterion, optimizer, device, config)\n",
      "File \u001b[1;32mc:\\Users\\Cyril\\Desktop\\Code\\MIPLab-TeamCEE-DeepLearningforBiomed\\notebooks\\../code\\training.py:107\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m(epochs, model, train_loader, valid_loader, criterion, optimizer, device, config)\u001b[0m\n\u001b[0;32m    104\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m total_loss_c\u001b[39m.\u001b[39mitem()\n\u001b[0;32m    106\u001b[0m     total_loss_c\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m--> 107\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m    109\u001b[0m train_loss_total \u001b[39m=\u001b[39m total_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_loader)\n\u001b[0;32m    110\u001b[0m train_loss_si \u001b[39m=\u001b[39m loss_si \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_loader)\n",
      "File \u001b[1;32mc:\\Users\\Cyril\\anaconda3\\envs\\DLbiomed\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Cyril\\anaconda3\\envs\\DLbiomed\\lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\Cyril\\anaconda3\\envs\\DLbiomed\\lib\\site-packages\\torch\\optim\\adamw.py:171\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    158\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m\"\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    160\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    161\u001b[0m         group,\n\u001b[0;32m    162\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    168\u001b[0m         state_steps,\n\u001b[0;32m    169\u001b[0m     )\n\u001b[1;32m--> 171\u001b[0m     adamw(\n\u001b[0;32m    172\u001b[0m         params_with_grad,\n\u001b[0;32m    173\u001b[0m         grads,\n\u001b[0;32m    174\u001b[0m         exp_avgs,\n\u001b[0;32m    175\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    176\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    177\u001b[0m         state_steps,\n\u001b[0;32m    178\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    179\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    180\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    181\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    182\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    183\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    184\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    185\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    186\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    187\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    188\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    189\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    190\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    191\u001b[0m     )\n\u001b[0;32m    193\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\Cyril\\anaconda3\\envs\\DLbiomed\\lib\\site-packages\\torch\\optim\\adamw.py:321\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adamw\n\u001b[1;32m--> 321\u001b[0m func(\n\u001b[0;32m    322\u001b[0m     params,\n\u001b[0;32m    323\u001b[0m     grads,\n\u001b[0;32m    324\u001b[0m     exp_avgs,\n\u001b[0;32m    325\u001b[0m     exp_avg_sqs,\n\u001b[0;32m    326\u001b[0m     max_exp_avg_sqs,\n\u001b[0;32m    327\u001b[0m     state_steps,\n\u001b[0;32m    328\u001b[0m     amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    329\u001b[0m     beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    330\u001b[0m     beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    331\u001b[0m     lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    332\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    333\u001b[0m     eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    334\u001b[0m     maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    335\u001b[0m     capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    336\u001b[0m     differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    337\u001b[0m     grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    338\u001b[0m     found_inf\u001b[39m=\u001b[39;49mfound_inf,\n\u001b[0;32m    339\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Cyril\\anaconda3\\envs\\DLbiomed\\lib\\site-packages\\torch\\optim\\adamw.py:390\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    389\u001b[0m exp_avg\u001b[39m.\u001b[39mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[1;32m--> 390\u001b[0m exp_avg_sq\u001b[39m.\u001b[39;49mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad, value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[0;32m    392\u001b[0m \u001b[39mif\u001b[39;00m capturable \u001b[39mor\u001b[39;00m differentiable:\n\u001b[0;32m    393\u001b[0m     step \u001b[39m=\u001b[39m step_t\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "#         training\n",
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "# change to cuda\n",
    "device = \"cpu\"\n",
    "training_loop(config[\"epochs\"], model, train_loader, test_loader, criterion, optimizer, device, config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2af70ddca1e214ae879d4eaa8be4e90c6947a1c72d95a69bb7122dd7b7c88083"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
