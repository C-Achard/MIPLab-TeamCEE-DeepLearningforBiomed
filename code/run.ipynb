{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "#         imports\n",
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "from training import balanced_data_shuffle, training_loop, evaluate\n",
    "from utils import get_dict_raw_data\n",
    "from models import MRIVisionTransformers\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "#         hyperparameters\n",
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "config = {\n",
    "    # general\n",
    "    \"epochs\": 100,\n",
    "    \"batch_size\": 4,\n",
    "    \"lr\": 1e-3,\n",
    "\n",
    "    # model\n",
    "    \"d_model_output\": 512,\n",
    "    \"d_model_input\": 400,\n",
    "    \"dropout\" : 0.1,\n",
    "    \"attention_dropout\" : 0.1,\n",
    "    \"num_heads\": 4,\n",
    "    \"num_layers\": 0, # TBA?\n",
    "\n",
    "    # optimizer\n",
    "    \"lambda_si\": 0.6,\n",
    "    \"lambda_td\": 0.4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "#         subject ID list\n",
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "IDs = [100307,  117122,  131722,  153025,  211720,\n",
    "100408,  118528,  133019,  154734,  212318,      \n",
    "101107,  118730,  133928,  156637,  214423,        \n",
    "101309,  118932,  135225,  159340,  221319,       \n",
    "101915,  120111,  135932,  160123,  239944 ,      \n",
    "103111,  122317,  136833,  161731,  245333,        \n",
    "103414,  122620,  138534,  162733,  280739,        \n",
    "103818, 123117,  139637,  163129,  298051,        \n",
    "105014,  123925,  140925,  176542,  366446,        \n",
    "105115,  124422,  144832,  178950,  397760,        \n",
    "106016,  125525,  146432,  188347,  414229,        \n",
    "108828,  126325,  147737,  189450,  499566,\n",
    "110411,  127630,  148335,  190031,  654754,\n",
    "111312,  127933,  148840,  192540,  672756,\n",
    "111716,  128127,  149337,  196750,  751348,\n",
    "113619,  128632,  149539,  198451,  756055,\n",
    "113922,  129028,  149741,  199655,  792564,\n",
    "114419,  130013,  151223,  201111,  856766,\n",
    "115320,  130316,  151526,  208226,  857263]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "#         joining train and test dataframes from all subjects\n",
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# \"C:/Users/emy8/OneDrive/Documents/EPFL/Master/MA3/DeepLbiomed/Project/DATA/\", \"1003\" \n",
    "\n",
    "data_dict_train, data_dict_test = get_dict_raw_data(\"/Users/eddyvonmatt/Desktop/MIPLab-TeamCEE-DeepLearningforBiomed-main/DATA/\", IDs[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label_id     task_id                                                mat\n",
      "0    100307    GAMBLING  [[0.2356709594115424, 0.03883497545044236, 0.1...\n",
      "1    100307       REST1  [[0.21854491103466994, 0.07509374392964863, 0....\n",
      "2    100307       MOTOR  [[0.2141270371266362, 0.040754342863046, 0.084...\n",
      "3    100307    LANGUAGE  [[0.2317390561241142, 0.06537822245634475, 0.0...\n",
      "4    100307      SOCIAL  [[0.27075755129825896, 0.07942572217389814, 0....\n",
      "5    100307       REST2  [[0.2509722712619662, 0.06429771271159306, 0.1...\n",
      "6    100307          WM  [[0.28122430896568573, 0.12358947079320645, 0....\n",
      "7    100307     EMOTION  [[0.27626702525883573, 0.03827488524289221, 0....\n",
      "8    100307  RELATIONAL  [[0.2709434948110919, 0.08915439190003989, 0.1...\n",
      "9    117122       REST2  [[0.38164660255395016, 0.24027807540608934, 0....\n",
      "10   117122     EMOTION  [[0.2922083137463598, 0.04759277700431855, 0.1...\n",
      "11   117122    GAMBLING  [[0.25858517393823743, 0.05327319230138714, 0....\n",
      "12   117122      SOCIAL  [[0.2973511054200175, 0.13792407573291332, 0.1...\n",
      "13   117122          WM  [[0.3465619339213891, 0.15751164382201774, 0.2...\n",
      "14   117122       MOTOR  [[0.27314676241355523, 0.09901914239052637, 0....\n",
      "15   117122       REST1  [[0.29054118390508005, 0.0965628709575545, 0.1...\n",
      "16   117122    LANGUAGE  [[0.22859935518482, 0.07709724110848372, 0.042...\n",
      "17   117122  RELATIONAL  [[0.3394606264697297, 0.1586822745157625, 0.22...\n",
      "18   131722      SOCIAL  [[0.2681279919433658, 0.11117226373872925, 0.1...\n",
      "19   131722          WM  [[0.31459010669100723, 0.13952760186306779, 0....\n",
      "20   131722       REST1  [[0.26225288630457094, 0.12855069083006654, 0....\n",
      "21   131722       MOTOR  [[0.37369040083993843, 0.21442582521709871, 0....\n",
      "22   131722     EMOTION  [[0.22052439092699064, 0.052630642343204775, 0...\n",
      "23   131722  RELATIONAL  [[0.24997576174491226, 0.01015263823236376, 0....\n",
      "24   131722       REST2  [[0.30565818034609404, 0.16903926490602952, 0....\n",
      "25   131722    GAMBLING  [[0.28066871754258227, 0.10278064681389672, 0....\n",
      "26   131722    LANGUAGE  [[0.21610098503082167, 0.05447112484814826, 0....\n"
     ]
    }
   ],
   "source": [
    "print(data_dict_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "#         label encoding\n",
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# one hot encoding\n",
    "if False:\n",
    "    enc_labels = OneHotEncoder(handle_unknown='ignore')\n",
    "    enc_tasks = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "    enc_labels.fit(data_dict_train[\"label_id\"].to_numpy().reshape(-1, 1))\n",
    "    enc_tasks.fit(data_dict_train[\"task_id\"].to_numpy().reshape(-1, 1))\n",
    "\n",
    "    enc_train_label_encodings = enc_labels.transform(data_dict_train[\"label_id\"].to_numpy().reshape(-1, 1)).toarray()\n",
    "    enc_train_task_encodings = enc_tasks.transform(data_dict_train[\"task_id\"].to_numpy().reshape(-1, 1)).toarray()\n",
    "\n",
    "    enc_test_label_encodings = enc_labels.transform(data_dict_test[\"label_id\"].to_numpy().reshape(-1, 1)).toarray()\n",
    "    enc_test_task_encodings = enc_tasks.transform(data_dict_test[\"task_id\"].to_numpy().reshape(-1, 1)).toarray()\n",
    "\n",
    "    data_dict_train[\"enc_label_id\"] = enc_train_label_encodings.tolist()\n",
    "    data_dict_train[\"enc_task_id\"] = enc_train_task_encodings.tolist()\n",
    "\n",
    "    data_dict_test[\"enc_label_id\"] = enc_test_label_encodings.tolist()\n",
    "    data_dict_test[\"enc_task_id\"] = enc_test_task_encodings.tolist()\n",
    "\n",
    "# label encoding\n",
    "enc_labels = LabelEncoder()\n",
    "enc_tasks = LabelEncoder()\n",
    "\n",
    "enc_labels.fit(data_dict_train[\"label_id\"].tolist())\n",
    "enc_tasks.fit(data_dict_train[\"task_id\"].tolist())\n",
    "\n",
    "enc_train_label_encodings = enc_labels.transform(data_dict_train[\"label_id\"].tolist())\n",
    "enc_train_task_encodings = enc_tasks.transform(data_dict_train[\"task_id\"].tolist())\n",
    "\n",
    "enc_test_label_encodings = enc_labels.transform(data_dict_test[\"label_id\"].tolist())\n",
    "enc_test_task_encodings = enc_tasks.transform(data_dict_test[\"task_id\"].tolist())\n",
    "\n",
    "data_dict_train[\"enc_label_id\"] = enc_train_label_encodings\n",
    "data_dict_train[\"enc_task_id\"] = enc_train_task_encodings\n",
    "data_dict_test[\"enc_label_id\"] = enc_test_label_encodings\n",
    "data_dict_test[\"enc_task_id\"] = enc_test_task_encodings\n",
    "\n",
    "#enc.inverse_transform() to reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w9/94vqs82s3gq0qywpffjb890w0000gn/T/ipykernel_22897/1234970636.py:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:248.)\n",
      "  train_dataset = TensorDataset(torch.tensor(data_dict_train[\"mat\"][:]).float(), torch.tensor(data_dict_train[\"enc_label_id\"][:]), torch.tensor(data_dict_train[\"enc_task_id\"][:]))\n"
     ]
    }
   ],
   "source": [
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "#         initializing dataloader objects\n",
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(data_dict_train[\"mat\"][:]).float(), torch.tensor(data_dict_train[\"enc_label_id\"][:]), torch.tensor(data_dict_train[\"enc_task_id\"][:]))\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(torch.tensor(data_dict_test[\"mat\"][:]).float(), torch.tensor(data_dict_test[\"enc_label_id\"][:]), torch.tensor(data_dict_test[\"enc_task_id\"][:]))\n",
    "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 400, 400])\n"
     ]
    }
   ],
   "source": [
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "#         initializing model\n",
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "model = MRIVisionTransformers(\n",
    "        output_size = config[\"d_model_output\"],\n",
    "        input_size = config[\"d_model_input\"],\n",
    "        num_heads = config[\"num_heads\"],\n",
    "        dropout = config[\"dropout\"],\n",
    "        attention_dropout = config[\"attention_dropout\"]\n",
    ")\n",
    "\n",
    "x = torch.randn(1, 400, 400)\n",
    "y = model(x)\n",
    "\n",
    "# x_si, x_td, attn_weights\n",
    "print(y[0].size())\n",
    "print(y[1].size())\n",
    "print(y[2].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 - loss_total: 4.2294 - acc: 0.0000 - val-loss_total: 3.4689 - val-acc: 0.0000 (17.57s/epoch)\n",
      "Epoch: 2/100 - loss_total: 2.2377 - acc: 0.0000 - val-loss_total: 2.9964 - val-acc: 0.0000 (13.21s/epoch)\n",
      "Epoch: 3/100 - loss_total: 2.1823 - acc: 0.0000 - val-loss_total: 1.3685 - val-acc: 0.0000 (12.67s/epoch)\n",
      "Epoch: 4/100 - loss_total: 1.1520 - acc: 0.0000 - val-loss_total: 1.7359 - val-acc: 0.0000 (11.70s/epoch)\n",
      "Epoch: 5/100 - loss_total: 0.9093 - acc: 0.0000 - val-loss_total: 2.2340 - val-acc: 0.0000 (14.06s/epoch)\n",
      "Epoch: 6/100 - loss_total: 0.9250 - acc: 0.0000 - val-loss_total: 2.1779 - val-acc: 0.0000 (13.16s/epoch)\n",
      "Epoch: 7/100 - loss_total: 0.6000 - acc: 0.0000 - val-loss_total: 1.8633 - val-acc: 0.0000 (12.39s/epoch)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39m# change to cuda\u001b[39;00m\n\u001b[1;32m      9\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m training_loop(config[\u001b[39m\"\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m\"\u001b[39m], model, train_loader, test_loader, criterion, optimizer, device, config)\n",
      "File \u001b[0;32m~/Desktop/MIPLab-TeamCEE-DeepLearningforBiomed/code/training.py:90\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(epochs, model, train_loader, valid_loader, criterion, optimizer, device, config)\u001b[0m\n\u001b[1;32m     87\u001b[0m     loss_td \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_td_c\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     88\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m total_loss_c\u001b[39m.\u001b[39mitem()\n\u001b[0;32m---> 90\u001b[0m     total_loss_c\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     91\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     93\u001b[0m train_loss_total \u001b[39m=\u001b[39m total_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_loader)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39m_execution_engine\u001b[39m.\u001b[39mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, accumulate_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "#         training\n",
    "###-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "# change to cuda\n",
    "device = \"cpu\"\n",
    "training_loop(config[\"epochs\"], model, train_loader, test_loader, criterion, optimizer, device, config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2af70ddca1e214ae879d4eaa8be4e90c6947a1c72d95a69bb7122dd7b7c88083"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
